---
title: 'ML From Scratch, Part 5: Gaussian Mixture Models'
author: Oran Looney
date: 2019-01-20
tags:
  - Python
  - Statistics
  - From Scratch
  - Machine Learning
image: /post/ml-from-scratch-part-5-gmm_files/lead.jpg
draft: true
---

This one is quite a bit different than early entries in the series
because not only are we switching contexts from supervised to
unsupervised learning, but we are also moving past gradient
descent to one of the neatest and most versatile algorithms
out there: the EM Algorithm. 

When I first understood the EM Algorithm and realized I could
now understand and implement latent variable models, it was
like that scene in Aladdin where they're singing "It's a whole new world..."
Gradient descent may be the engine that powers deep learning,
but the EM algorithm is what powers factor analysis, etc. TODO

TODO: k-means

[EM]: https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm
