---
title: 'ML From Scratch, Part 4: Decision Trees'
author: Oran Looney
date: 2019-03-01
tags:
  - Python
  - Statistics
  - From Scratch
  - Machine Learning
image: /post/ml-from-scratch-part-4-decision-tree_files/lead.jpg
draft: true
---

So far we've followed one particular thread. Linear regression
via OLS is the oldest and most well known model. By wrapping
it in a non-linear function which clamped its output to the
range $[0, 1]$ we made it suitable for classification problems
as well. However this model, which we called logistic regression,
was still linear in its parameters and therefore not able to
represent arbitrary non-linear functions. We fixed that by
stacking several logistic regression models on top of one
another; the resulting model was expressive enough to represent
arbitrary non-linear responses and interactions between variables
but could still be trained by gradient descent.

Today we sweep away that history and present a model that
is explicitly designed to model non-linearities and interactions
directly. 

Greedy algorithm

What happens if we're wrong? What if this isn't the best possible cut point
after we consider future decisions as well? Is our final model worse?
No, not at all. All that happens is that it will take a few extra cuts
to fully classify the data and our tree will be a little deeper. 


Gini Impurity and Information Gain
----------------------------------

Gini impurity is defined as the probability that a randomly chosen element
would be misclassified:

\[
    I_G(\mathbf{p}) = \sum_{i} \mathbf{p}_i \sum_{j \neq i} \mathbf{p}_j 
\]

In the binary case, we usually simplify the notation and refer
to $\mathbf{p}_1 = p$ and $\mathbf{p}_2 = 1-p$, yielding this
much simpler expression:

\[
    I_G(p) = p(1-p) + (1-p)p = 2p(1-p)
\]

TODO: information gain, comparison


Finding The Best Cut Point
--------------------------

At each stage, we have two decisions to make: which feature to use
for the cut, and the exact value to cut out. Each rule is of the
form

\[
    X_i \leq C
\]

While it would be possible to simply brute force our way through
all possible cut points, calculating gini impurity from scratch
each and every time, this is hugely slower than a more efficient
but slightly harder to understand vectorized algorithm.

    def best_split_point(X, y, column):
        # sorting y by the values of X makes
        # it almost trivial to count classes for
        # above and below any given candidate split point. 
        ordering = np.argsort(X[:,column])
        classes = y[ordering]

        # these vectors tell us how many of each
        # class are present "below" (to the left)
        # of any given candidate split point. 
        class_0_below = (classes == 0).cumsum()
        class_1_below = (classes == 1).cumsum()
        
        # Subtracting the cummulative sum from the total
        # gives us the reversed cummulative sum. These
        # are how many of each class are above (to the
        # right) of any given candidate split point.
        #
        # Because class_0_below is a cummulative sum
        # the last value in the array is the total sum.
        # That means we don't need to make another pass
        # through the array just to get the total; we can
        # just grab the last element. 
        class_0_above = class_0_below[-1] - class_0_below
        class_1_above = class_1_below[-1] - class_1_below
        
        # below_total = class_0_below + class_1_below
        below_total = np.arange(1, len(y)+1)
        # above_total = class_0_above + class_1_above
        above_total = np.arange(len(y)-1, -1, -1)

        # we can now calculate gini impurity in a single
        # vectorized operation. The naive formula would be:
        #
        #     (class_1_below/below_total)*(class_0_below/below_total)
        # 
        # however, divisions are expensive and we can get this down
        # to only one division if we combine the denominator term.
        gini = class_1_below * class_0_below / (below_total ** 2) + \
               class_1_above * class_0_above / (above_total ** 2)

        gini[np.isnan(gini)] = 1
        
        # we need to reverse the above sorting to
        # get the rule into the form C_n < split_value. 
        best_split_rank = np.argmin(gini)
        best_split_gini = gini[best_split_rank]
        best_split_index = np.argwhere(ordering == best_split_rank).item(0)
        best_split_value = X[best_split_index, column]
        
        return best_split_gini, best_split_value, column


Building the Tree
-----------------

TODO: recursive partition algorithm

[RP]: https://en.wikipedia.org/wiki/Recursive_partitioning
[DC]: https://en.wikipedia.org/wiki/Divide-and-conquer_algorithm

Every node starts life as a leaf node, but when its `.split()` method is
called, it mutates into a branch node with two leaf nodes underneath.  The
split is made by calculating the optimal split point for each feature, then
choosing the feature and split point which minimizes gini impurity.  This
continues recursively for both children until a node is perfectly pure or the
maximum `depth` parameter is reached.

    class Node:
        def __init__(self, X, y):
            self.X = X
            self.y = y
            self.is_leaf = True
            self.column = None
            self.split_point = None
            self.children = None
        
        def is_pure(self):
            p = self.probabilities()
            if p[0] == 1 or p[1] == 1:
                return True
            return False

        def split(self, depth=0):
            X, y = self.X, self.y
            if self.is_leaf and not self.is_pure():
                splits = [ best_split_point(X, y, column) for column in range(X.shape[1]) ]
                splits.sort()
                gini, split_point, column = splits[0]
                self.is_leaf = False
                self.column = column
                self.split_point = split_point
                
                below = X[:,column] <= split_point
                above = X[:,column] > split_point 
                
                self.children = [
                    Node(X[below], y[below]),
                    Node(X[above], y[above])
                ]
                
                if depth:
                    for child in self.children:
                        child.split(depth-1)


To obtain predictions from our tree, we simply apply all the learned rules.
Given a new vector of predictors $x$, we start at the root node and then move
to either the left or right child based on the nodes decision rule. When we
reach a leaf, we can return a probability based on the proportion of classes in
the leaf. 

    def probabilities(self):
        return np.array([
            np.mean(self.y == 0),
            np.mean(self.y == 1),
        ])
    
    def predict_proba(self, row):
        if self.is_leaf:
            return self.probabilities()
        else:
            if row[self.column] <= self.split_point:
                return self.children[0].predict_proba(row)
            else:
                return self.children[1].predict_proba(row)

This prediction step can also be vectorized by applying a separate vectorized
filter for each leaf node. However, in a tree of depth $k$, this requires
calculating $2^k$ separate filters, each comprised of the logical AND of
$k$ separate comparisons. This is not usually faster than just applying
the rules row-by-row. 

Interface
---------

The above Node class can be used directly to fit models but as we've done
elsewhere in the series we give our model a user-friendly, scikit-learn style
interface:

    class DecisionTreeClassifier:
        def __init__(self, max_depth=3):
            self.max_depth = int(max_depth)
            self.root = None
            
        def fit(self, X, y):
            self.root = Node(X, y)
            self.root.split(self.max_depth)
            
        def predict_proba(self, X):
            results = []
            for row in X:
                p = self.root.predict_proba(row)
                results += [p]
            return np.array(results)
                
        def predict(self, X):
            return (self.predict_proba(X)[:, 1] > 0.5).astype(int)

Testing
-------

The breast cancer dataset it good for testing decision trees because it is
high dimensional and highly non-linear.

    # a small classification data set with 30 to get with. 
    breast_cancer = load_breast_cancer()
    X = breast_cancer.data
    y = breast_cancer.target

    model = DecisionTreeClassifier(max_depth=4)
    model.fit(X, y)
    y_hat = model.predict(X)
    p_hat = model.predict_proba(X)[:,1]

The models out-of-the-box (by which I mean, "without need for hyper-parameter selection via
cross-validation") performance is quite good:

    print(confusion_matrix(y, y_hat))
    print('Accuracy:', accuracy_score(y, y_hat))

                     True Class
                        P     N
     Predicted     P  193    19
     Class         N   18   339

     Accuracy: 0.9349736379613357

This confusion matrix and accuracy are only part of the story - in particular, they are
performance we see if we choose to define a positive test result as $p > 0.5$. We can get 
a broader view the models performance over a range of possible thresholds with an ROC curve:

![ROC Curve](/post/ml-from-scratch-part-4-decision-tree_files/auc.png)

An AUC of .96 is pretty respectable.

We can also look at the results as a function of the predictor variable $X$. Since there are 30 separate
features, we will just look at a representative sample. For each pair of predictor variables, we'll plot
true positives in green, true negatives in blue, and misclassifications in red.

    plt.figure(figsize=(16,30))
    markers = ['o', 'x']
	red = (1, 0.2, 0.2, 0.5)
	green = (0.3, 0.9, 0.3, 0.3)
	blue = (0.2, 0.4, 0.8, 0.3)

    for i in range(28):
        plt.subplot(7, 4, i+1)
        for cls in [0, 1]:
            mask = (y == cls) & (y == y_hat)
            plt.scatter(
                x=X[mask,i], 
                y=X[mask,i+1], 
                c=[blue if positive else green for positive in y[mask]],
                marker=markers[cls]
            )
            mask = (y == cls) & (y != y_hat)
            plt.scatter(
                x=X[mask,i], 
                y=X[mask,i+1], 
                c=red,
                marker=markers[cls],
                zorder=10
            )

![Decision Tree Pairs](/post/ml-from-scratch-part-4-decision-tree_files/pairs.png)

A simple text-based visualization of our tree can be done by
adding a `formatted()` method to the `Node()` class:

    def formatted(self, indent=0):
        if self.is_leaf:
            s = "Leaf({p[0]:.3f}, {p[1]:.3f})".format(p=self.probabilities())
        else:
            s = "Branch(X{column} <= {split_point})\n{left}\n{right}".format(
                column=self.column, 
                split_point=self.split_point,
                left=self.children[0].formatted(indent+1),
                right=self.children[1].formatted(indent+1))
            
        return "    " * indent + s

    def __str__(self):
        return self.formatted()
    
    def __repr__(self):
        return str(self)

The breast cancer decision tree has the following structure,
where greater indentation corresponds to greater depth in the tree.


    Branch(X22 <= 89.04)
        Branch(X6 <= 0.0)
            Leaf(0.000, 1.000)
            Branch(X16 <= 0.0009737)
                Leaf(0.000, 1.000)
                Branch(X16 <= 0.001184)
                    Leaf(0.000, 1.000)
                    Branch(X19 <= 0.004651)
                        Leaf(0.013, 0.987)
                        Leaf(0.000, 1.000)
        Branch(X22 <= 96.42)
            Branch(X6 <= 0.004559)
                Leaf(0.000, 1.000)
                Branch(X6 <= 0.01063)
                    Leaf(0.000, 1.000)
                    Branch(X9 <= 0.05913)
                        Leaf(0.059, 0.941)
                        Leaf(0.086, 0.914)
            Branch(X26 <= 0.3169)
                Branch(X22 <= 117.7)
                    Branch(X14 <= 0.006133)
                        Leaf(0.109, 0.891)
                        Leaf(0.273, 0.727)
                    Branch(X19 <= 0.002581)
                        Leaf(0.875, 0.125)
                        Leaf(1.000, 0.000)
                Branch(X20 <= 27.32)
                    Branch(X20 <= 27.32)
                        Leaf(0.902, 0.098)
                        Leaf(0.000, 1.000)
                    Leaf(1.000, 0.000)

Note that several paths down the tree lead to immediately to large, totally
pure leaf nodes.  That's because in this particular dataset, there are large
regions of the input space which can be unambiguously classified. However, as
we get closer to the true decision boundary, the predictions become more
probabilistic, and we may only be able to say that perhaps 87.5% of cases will
be negative.

Conclusion
----------

Today we saw an incredibly simple and obvious algorithm tackle a
seriously difficult problem and achieve surprisingly good out-of-the-box 
performance. Decision trees are somewhat data-hungry: to improve performance
without overfitting we would need to add more nodes to our model, but each
layer that we add more than doubles the amount of data we need before our
leaves have too few data points to reliably split. On this small dataset,
we can't go beyond three or four layers.

While decision trees are occasionally used directly on datasets, their real
importance is in their use as the main ingredient in two state-of-the-art ML
algorithms, namely [random forest][RF] and [extreme gradient boosting][XGB].

There are some good arguments that suggest that any weak learner algorithm
can be turned into a strong leaner when combined together. TODO: WL1-3

The decision boundary of a decision tree is a series of axis-aligned straight
line segments. This is rarely a well-motivated boundary that would be expected
to generalize well. With a very deep tree, a diagonal or curved boundary can
be approximated, yet this can require a large amount of data close to the decision
boundary. However, decision trees do very well when given discrete features.


[2C]: https://projecteuclid.org/euclid.ss/1009213726
[CART]: https://www.amazon.com/Classification-Regression-Wadsworth-Statistics-Probability/dp/0412048418
[RF]: https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf
[XGB]: https://blog.exploratory.io/introduction-to-extreme-gradient-boosting-in-exploratory-7bbec554ac7


[WL1]: http://rob.schapire.net/papers/strengthofweak.pdf
[WL2]: https://www.cis.upenn.edu/~mkearns/papers/boostnote.pdf
[WL3]: http://www.face-rec.org/algorithms/Boosting-Ensemble/decision-theoretic_generalization.pdf


