---
title: 'ML From Scratch, Part 2: Logistic Regression'
author: Oran Looney
date: 2018-12-05
tags:
  - Python
  - Statistics
  - From Scratch
  - Machine Learning
image: /post/ml-from-scratch-part-2-logistic-regression_files/lead.jpg
draft: true
---

In this second second installment of the [machine learning from scratch[MLFS]
we switch the point of view from *regression* to *classification*: instead of
estimating a number, we will be trying to guess which of 2 possible classes a
given input belongs to. A classic example is looking at a photo and deciding if
its a [cat or a dog][COD]. 

In practice, its extremely common to need to decide between $k$ classes where
$k > 2$ but in this series we'll limit ourselves to just two classes - the
so-called binary classification problem - because generalizations to many
classes are usually both tedious and straight-forward.  In fact, even if the
algorithm doesn't naturally generalize beyond binary classification (looking at
you, [SVM][SVM]) there's a general strategy for turing any binary
classification algorithm into a multiclass classification algorithm called
[one-vs-all][OVA]. So we can safely set aside the complexities of the multiclass
problem and focus on binary classification for this series.

The binary classification is extremely central in machine learning and in this
series we'll be looking at no fewer than four different algorithms. In an
undergraduate machine learning class, you'd probably work through a few
algorithms that today only have historical or pedagogical value to dip your
toes in the water and get a feel for the binary classification problem: the
one-unit perceptron, linear discriminant analysis, and the winnow algorithm.
We will omit those and jump straight to the simplest classification that is
in widespread use: logistic regression. 

I say, "simplest," but most people don't think of LR as "simple." That's
because they're thinking of it use within the context of statistical analysis
and the analysis of experiments. In those contexts, there's a ton of associated
mathematical machinery that goes along with *validating* and *interpretting*
logistic regression models, and that stuff *is* complicated. A good book on
*that* side of logistic regression is [Applied Logistic Regression by Hosmer et
al.][ALR]. But within the context of machine learning, logistic regression is
an extremely simple predictive model: as we'll see, the heart of the algorithm
is only a few lines of code.

Nevertheless, it's important for two reasons. First, it can be suprisingly
effective. It's not uncommon for some state-of-the-art algorithm to
significantly contribute to global warming by running a hyperparameter grid
search over a cluster of GPU instances in the cloud, only to end up with a
final model with only marginally higher test set performance than the original.
This isn't always true: for example, LR tends to get only ~90% accuracy on the
MNIST handwritten digit classification problem, which is much lower than either
humans or deep learning. But in the many cases for which it *is* true, it's
worth asking if the problem is amenable to more advanced machine learning
techniques at all.

The second reason logistic regression is important is that it provides a
important conceptual foundation for neural networks and deep learning, which
we'll visit later in this series.

The Logistic Function
---------------------

The logistic function, also called the sigmoid function is given by

\[ \sigma(x) = \frac{1}{1 + e^{-x}} \]

and looks like so:

```{r echo=FALSE}
curve(1/(1+exp(-x)), xlim=c(-6,6))
title("Logistic Function")
```

We will want to get acquainted with this function before proceeding
as it has several interesting properties.

First, we note that while its domain is $(-\infty, +\infty)$, its
range is $(0, 1)$. Therefore its output will always be a valid
probability. Note also that $\sigma(0) = 0.5$ so if we interpret
the output as a probability, all negative numbers map to probabilities
that are unlikely, while all positive numbers map to probabilities that
are likely, while zero maps to exactly even odds.

A few lines of simple algebra will show that its inverse function,
also called the "logit" function, is given by

\[ \sigma^{-1}(x) = \ln{\frac{x}{1-x}} \]

Note that if $x = e^p$ where $p$ is a probability between 0 and 1,
then we have

\[ \sigma^{-1}(e^p) = \frac{p}{1-p} \]

Where the right hand side is an *odds ratio*. So one interpretation
of the logistic function is that it maps *log odds ratios* to *probabilities*.

For example, if something has a probability of 0.2, then it has 4:1 odds,
and a log odds ratio of $ln 4 = -1.39$. So $\sigma(0.2) = -1.39$.

The logistic function also has a pretty interesting derivative. The
easiest way to see this is to use implicit differentiation, but I'll
show a slightly longer but less magical derivation which only uses the
chain rule and basic algebra.

\[ \frac{d \sigma(x)}{dx} 
	= \frac{d}{dx} \frac{1}{1 + e^{-x}} 
	= \frac{-1}{( 1+ e^{-x})^2} -e^{-x}
	= \frac{1}{1 + e^{-x}} \frac{1 + e^{-x} - 1}{ 1+ e^{-x}}
	= \frac{1}{1 + e^{-x}} \big( \frac{1 + e^{-x}}{1+ e^{-x}} - \frac{1}{ 1+ e^{-x}} \big)
	= \frac{1}{1 + e^{-x}} \big( 1                            - \frac{1}{ 1+ e^{-x}} \big)
	= \sigma(x) ( 1 - \sigma(x) )
\]

So we can see that $\sigma'(x)$ can expressed as a simple quadratic function of
$\sigma(x)$.  It's not often that a derivative can be most conveniently
expressed in terms of the original function, but seems to be the case here.

Probability Regression
----------------------

Let's say $Y$ is a discrete random variable with support ${0, 1}$. The joint
probability distribution of $Y$ and $X$ is such that 

$F_{Y|X} \sim Bernoulli(p(x))$

$Y \sim Bernoulli(p(x))$.

\[ \hat{y} = E[Y|X] = P(Y=1|X) = \sigma(X\Theta) \]

\[ L(\Theta; X, y) = \prod_{i=1}{N} P(Y=1|X_i)^{y_i} P(Y=0|X_i)^{(1 - y_i)} \]

We're using a simple notational trick to write this compactly: because $y \in {0, 1}$,
The first factor will be reduced to a constant 1 if $y = 0$ and likewise the second term
will be reduced to a constant 1 if $y = 1$. So that's merely a compact way of writing
the two scenarios without an explicit if/else.

The log likelihood is simpler. 

\[ \ell(\Theta; X,y) = \ln L = \sum{i=1}{N} y_i \ln P(Y=1|X_i) + (1 - y_i) (1 - P(Y=1|X_i) \]

Using the above formula for our predictor $\hat{y}$:

\[ \ell(\Theta; X,y) = \ln L = \sum{i=1}{N} y_i \ln \hat{y}_i + (1 - y_i) (1 - \hat{y}_i \]

To find the MLE it suffices to minimize $\ell(\Theta; X, y)$ with respect to $\Theta$. Because
the function is once again smooth and convex, we can do this by finding the zero of the gradient.

\[ \frac{\partial \ell}{\partial \Theta_j} = \sum{i=1}{N} \frac{y_i}{\hat{y}_i} \frac{\partial \hat{y}_i}{\partial \Theta_j} + \frac{(1-y_i)}{(1-\hat{y}_i)} \frac{\partial \hat{y}_i}{\partial \Theta_j} \]

We can use our earlier lemma $\sigma'(x) = \sigma(x)(1-sigma(x))$ for the partial derivative of $\hat{y}$. 

Note also that because $\hat{y} = \sigma(X \Theta) = \sigma( X_0 \Theta_0 + X_1 \Theta_1 + ... + X_k \Theta_k )$, we will pick up an additional $X_j$ factor from the chain rule when differentiating by $\Theta_j$.

\[ \frac{\partial \ell}{\partial \Theta_j} = \sum{i=1}{N} \frac{y_i}{\hat{y}_i} \hat{y}_i (1-\hat{y}_i) X_j - \frac{(1-y_i)}{(1-\hat{y}_i)} \hat{y}_i (1-\hat{y}_i) X_j \]

Several cancellations occur and we are left with:

\[ \frac{\partial \ell}{\partial \Theta_j} = \sum{i=1}{N} \X_j ( y_i (1 - \hat{y}_i) - (1 - y_i) \hat{y}_i ) 
 = \sum{i=1}{N} \X_j ( y_i  - y_i \hat{y}_i) - \hat{y}_i + y_i \hat{y}_i)
 = \sum{i=1}{N} \X_j ( y_i  - \hat{y}_i )
\]

If we define $\mathbb{1} = (1, 1, ..., 1)$ to be a vector consisting of all ones and use $\circ$ to denote the Hadamard product we can write this is a compact vectorized form:

\[ \nabla_{\Theta} \ell(\Theta; X, y) = (X \circ  (y - \hat{y}))^T \mathbb{1} \]

This is almost suspiciously neat, but it's not really an accident. The reason why we chose the logistic function in the first place was precisely so this neat result
would drop out at the end. This is because the logistic function is the [canonical link function][CLF] for the Bernoulli distribution.

This raises a fairly interesting point. Is there any fundamental reason to believe the true population distribution *really* has a logistic link function, or is it
an arbitrary choice we made solely for mathematical convenience? As far as I can tell, it's the later case. More rigorous attempts to justify the choice of link
function end up making a different choice, such as probit regression which uses the CDF of the normal distribution. However, in practice these curves are extremely
similar, and if you showed me a plot of each side-by-side I could not for the life of me tell you which is which.

```{r, echo=FALSE}
curve(1/(1+exp(-x)), xlim=c(-6,6), col='blue', ylab='p', xaxt='n')
curve(pnorm(x/1.96), xlim=c(-6,6), add=TRUE, col='darkgreen')
title("Sigmoid vs. Normal CDF")
```

[CLF]: https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function

Gradient Descent
----------------









[ALR]: https://www.amazon.com/Applied-Logistic-Regression-David-Hosmer/dp/0470582472/
[SVM]: https://en.wikipedia.org/wiki/Support-vector_machine
[OVA]: http://mlwiki.org/index.php/One-vs-All_Classification
[COD]: https://www.kaggle.com/c/dogs-vs-cats
[MLFS]: /tags/from-scratch/
