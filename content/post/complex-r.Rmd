---
title: "Complex Numbers in R"
author: "Oran Looney"
date: 2018-06-17
tags: ["R", "Math"]
image: /post/complex-r_files/mandala.png
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment="#")
library(zoo)
```

R, like many scientific programming languages, has first-class support for
complex numbers.  And, just as in most other programming languages, this
functionality is ignored by the vast majority of users.

Yet complex numbers can often offer surprisingly elegant formulations and
solutions to problems. I want to convince you that familiarizing yourself with
R's excellent complex number functionality is well worth the effort and will
pay off tenfold in two different ways: first by showing you how they are so
amazingly useful you'll want to go out of your way to use them, and then by
showing you how they are so common and fundamental to modern analysis that you
couldn't avoid them if you wanted to. 

## Pythagorean Triples

Let's start with a problem which *could* be solved in other ways, but is greatly simplified
by the introduction of complex numbers that it almost seems magical.

A Pythagorean triple is an integer solution to the Pythagorean equation:

\[
a^2 + b^2 = c^2 \quad\quad a,b,c \in \mathbb{N}^+ \tag{1}
\]

You probably learned at least one of these in school -- the famous 3, 4, 5 triangle:

<img src="/post/complex-r_files/345.gif" alt="3-4-5 triangle">

In general [Diophantine equations][DIO] -- which require *integer* solutions --
can be quite hard to solve, so it might surprise you to hear that it's almost
trivially easy to write down an infinite of Pythagorean triples. Well, it's
easy if we use complex numbers, anyway.

A Gaussian integer is a complex number where both the real and imaginary parts
are integers:

\[
  Z[i] = \{ x + iv \mid x,y \in \mathbb{Z} \} \tag{2}
\]

So one way of stating the problem of finding all Pythagorean triples is to find
all Gaussian integers Which are an integer distance away from the origin. The
distance of a complex number from the origin is called its "norm" and denoted
$\lVert z \rVert$.

\[
  T = \{ z \in \mathbb{C} \mid \lVert z \rVert \in \mathbb{Z} \} \tag{3}
\]

Now, in general the norm of Gaussian integer will be the square root of an
integer (the integer $u^2 + v^2$ to be precise.) Therefore if we square a
Gaussian integer, it will have an integer norm and therefore represent a
Pythagorean triple!

\[
\forall z \in Z[i], z^2 \in T \tag{4}
\]

So that's a pretty good start: just a few minutes work, and we've already found
an *infinite number* of Pythagorean triples, and we have a computationally
trivial way of constructing new triples: we simply pick any two positive
integers $u$ and $v$ and then square the complex number $u + iv$.

Before address the more difficult question of whether or not we've found all
possible Pythagorean triples using this construction, let's switch over to R
and do some code our solution so far.

Our algorithm first requires us to pick pairs of positive integers. Just to be
thorough, we'll take all such pairs up to an arbitrary threshold.

Now, if we wanted just one or two complex numbers, we could use the literal syntax:

```{r literal}
triples <- c( 3+4i, 5+12i, 9+12i )
```

But since we want them along a grid, we'll use the `complex()` constructor. The
constructor is vectorized so by passing in two vectors of equal length we can
construct a one-dimensional vector of complex numbers.

```{r grid}
n = 400
grid <- expand.grid(u=1:(2*n), v=1:(2*n))
grid <- grid[ grid$u > grid$v, ]
gaussian_integers <- complex(real=grid$u, imaginary=grid$v)
```

Per the theoretical discussion above, we can generate Pythagorean triples by
simply squaring these.  All primitive math functions in R work just as well on
complex numbers: `exp`, `log`, `sin`, `cos`and of course exponentiation:

```{r squares}
triples <- gaussian_integers^2

# display the 10 with the smallest norm
cat( triples[order(Mod(triples))][1:10], sep="\n")
```

Did it work? We're certainly seeing some familiar pairings there, like $5+12i$
which maps to well-known triple $(5,12,13)$. To visualize them, we can simply
pass our complex vector to R's `plot()` function - it will conveniently plot
them in the complex plane for us! 

```{r triples}
triples <- triples[ Re(triples) <= n & Im(triples) <= n ]

# helper function to colorize complex points by their angle.
argcolor <- function(z) hsv(Arg(z)*2/pi, s=0.9, v=0.8)

plot(
  triples, 
  col=argcolor(triples),
  pch=20,
  xlim=c(0,n),
  ylim=c(0,n),
  main=paste("Squared Gaussian Integers Up to", n)
)
```

Now it turns out that our algorithm does not, in fact, generate all possible
triples. One thing it misses are multiples: if $(3,4,5)$ is a triple, then
$(6,8,10)$ should be a triple, and $(9,12,15)$ should be a triple, and so on.
So we have to expand our set to have all multiples.

```{r multiples}

multiples <- lapply(1:(floor(n/3)), function(m) triples*m)
triples <- unique(do.call(c, multiples))
```

It also turns out that in the special case where both integers are even we can
divide by two and get a new triple that was missed by the initial net we cast.
But that's the end of the special cases -- with this final rule in place, we're
now guaranteed to hit *every* Pythagorean triple.

```{r halves}
halves <- triples[ Re(triples) %% 2 == 0 & Im(triples) %% 2 == 0 ] / 2
triples <- unique(c(triples, halves))
```

Now all we need to is clean up duplicates and duplicate along the mirror line
of symmetry...

```{r cleanup}
triples <- triples[ Re(triples) <= n & Im(triples) <= n]
triples <- c(triples, complex(real=Im(triples), imaginary=Re(triples)))
```

..and we're finally ready to visualize the real solution.

```{r plotall}
plot(triples, col=argcolor(triples), pch=20)
title(paste("All Pythagorean Triples Up to", n))
```

That's too many to really understand, although there are definitely
patterns emerging. Let's zoom in and just plot a small region,but with more
detail.

```{r plotsmall}
small_n = 25
small_triples <- triples[ Re(triples) < small_n & Im(triples) < small_n ]
small_triples <- small_triples[ order(Mod(small_triples), decreasing=TRUE) ]

# plot points
plot(
  small_triples, 
  pch=20,
  ylim=c(0,small_n), 
  xlim=c(0,small_n),
  ylab="b", xlab="a")

# add triangles. Can't rely on automatic complex plane plotting here.
segments(
  Re(small_triples), Im(small_triples), 
  0, 0, 
  col=argcolor(small_triples))
segments(
  Re(small_triples), Im(small_triples), 
  Re(small_triples), 0, 
  col=argcolor(small_triples))
segments(
  Re(small_triples), 0, 
  0, 0, 
  col=argcolor(small_triples))

# points again, so that they're in the foreground.
points(small_triples, pch=20, col=argcolor(triples), cex=1)

# text label for the points
text(
  x=small_triples + 1i, 
  cex=0.8,
  labels=paste0(
    "(", 
    Re(small_triples), 
    ",",
    Im(small_triples),
    ",",
    Mod(small_triples),
    ")"
  )
)
title(paste("Pythagorean Triples Up to", small_n))
```

On the zoomed in view we can see each Pythagorean triple represented as a right
triangle; we can see that the integer multiples of solutions form a series of
similar triangles; we can see that there's a strong symmetry with every triple
$(a,b,c)$ having a partner $(b,a,c)$ which is its mirror reflection about the
like $y=x$.

From the zoomed out view we can see that the region close to either axis is
essentially devoid of solutions; that their radii (e.g. close to 50) where
triples are dense, and others where they are rare to non-existent; and it looks
as if triples actually get less dense as we move away from the origin.

This last observation, about triples thinning out as we move away from the
origin, can be understood and quantified by once again using the complex plane.
Triples are more or less the squares of Gaussian integers; we can say the
number of triples with norm less than $r$ is roughly proportional to the number
of Gaussian integers in the first quadrant and inside a circle with radius
$\sqrt{r}$, which is roughly proportional to the area of the quarter-circle of
radius $\sqrt{r}$, which is $\frac{\pi r}{4}$ or very roughly just $r$.

## Complex Roots and Eigenvalues

> Some problems are specific to complex numbers, some problems can be made
> easier by a complex representation, and some problems have complex numbers
> thrust upon them.
>
> -- <cite>William Shakespeare, 12 + 5i Night</cite>

The Pythagorean Triples gave us an opportunity to dip our toes in the water by
explicitly create some complex
numbers and manipulating them in fairly basic ways. Next let's look at
a some examples where complex are less of a choice an more of a necessity.
One such case that is of interest to statisticians and scientists (I'm
assuming you're not using R for embedded systems or game development) is
solving the [eignproblem][EGN] for a non-symmetric matrix.

Now, if your only exposure to eigenvalues is through [PCA][PCA], you might not
even be aware that eigenvalues are usually complex numbers... even when
the original matrix is comprised only of real numbers! However 
PCA is actually a very special case: a covariance matrix is *always*
a symmetric, positive-definite, real-valued matrix, therefore its
eigenvalues are always positive real numbers. 

However, there are plenty of situations in statistics where a non-symmetric
matrix arises naturally and the eigenvalues can give us deep insight into
the problem. Two such are [Markov Chains][MKV] and [AR models][AR]. Let's
only look at a simple example of an AR model - that will suffice to
demonstrate R's complex number functionality in this domain. 

Let's start by constructing a small time series that exhibits very strong
autocorrelation. To get some interesting behavior, I will give it a strongly
positive one day correlation, but then reverse it the next day. This should
give us both decay and oscillations.

```{r ts}
set.seed(43)
t_0 <- zoo(rnorm(n=100))
t_1 <- lag(t_0, k=1, na.pad=TRUE)
t_2 <- lag(t_0, k=2, na.pad=TRUE)
t_3 <- lag(t_0, k=3, na.pad=TRUE)
t <- na.omit(t_0 + 0.7*t_1 - 0.2*t_2 + 0.2*t_3)
plot(t, type='l')
title('Time Series With Autocorrelation')
pacf(t) # Partial Autocorrelation Plot
```

Next we construct the model. While I normally recommend the [forecast][FOR]
package, we'll just use the built-in `ar()` function today.

```{r ar}
ar_model <- ar(t)
ar_model
```

That's roughly what we'd expect based on how we constructed the time series and
what we saw on the partial autocorrelation plot: A strong positive
autocorrelation at lag one, a slightly less strong negative autocorrelation at
lag 2, then some harmonics. 

```{r roots}
ar_coefs <- ar_model$ar  # coefficients(ar_model) doesn't work, IDK why
roots <- polyroot( c(1,-ar_coefs) )
roots
plot(
  1/roots, 
  ylim=c(-1,1), 
  asp=1,
  main="Inverse AR Roots",
  panel.first=c(
    lines(complex(modulus=1, argument=0.01*2*pi)^(0:100), col='grey'),
    abline(h=0, col='grey'),
    abline(v=0, col='grey')
  )
)
```

Just to be clear, we're plotting the *inverse* roots, so we'd expect them to be
*inside* the unit circle if the process is stationary.

(Just as an Easter egg, we also used complex numbers to plot the unit circle.
If you're not sure how that worked, just remember that multiplying complex
numbers adds their arguments -- their angle with the x-axis -- together.)

Just from looking at the roots and observing that some are far from the real
axis, we can also say that this time series will experience a back-and-forth
oscillations as each day tries to "correct" for the previous day. If the
influence of history merely decayed away smoothly and exponentially, all the
roots would have been close to the real axis. (It's a common misconception that
how long effects last is related to the *order* of the model; when in fact even
an AR(1) model can have a very long memory if it has its root close to 1.)

Plotting the inverse roots of ARIMA models is standard practice because it can
help you diagnose [non-stationary series][NSS] and near [unit roots][UR], both
of which can ruin the predictive power and interpretability of a model. There's
no getting away from the fact that a polynomial of degree two or higher might have
complex roots.

But there's another way of looking at an AR model - as a [discrete linear dynamical system.][LDS]
Let's call the value of our at the $n$-th step $t_n$. Then we can define our state vectors
to be

\[
\boldsymbol{t}_n = \begin{bmatrix}
  t_n \\
  t_{n-1} \\
  t_{n-2} \\
  t_{n-3} \\
  t_{n-4} \\
  \end{bmatrix}
\]

In other words, we just stack $t_n$ with it's first four lags. That may not seem
like an improvment, but now we can write 

\[
\boldsymbol{t}_{n+1} =\boldsymbol{F} \boldsymbol{t}_n
\]

or more explicitly:

\[
  \begin{bmatrix}
  t_{n+1} \\
  t_{n} \\
  t_{n-1} \\
  t_{n-2} \\
  t_{n-3} \\
  \end{bmatrix} = \boldsymbol{F}
  \begin{bmatrix}
  t_n \\
  t_{n-1} \\
  t_{n-2} \\
  t_{n-3} \\
  t_{n-4} \\
  \end{bmatrix}
\]

where $\boldsymbol{F}$ is the "forward time evolution"
matrix. This basically says we can always compute
the state of our time series at the next time step by applying a *linear* operator
to the prevous state. And in fact, we already have a good idea what the matrix 
$\boldsymbol{F}$ should look like. For one thing, it's clear that the four lagged
components can simply be grabbed from the old state by shifting down by one:

\[
  \begin{bmatrix}
  t_{n+1} \\
  t_{n} \\
  t_{n-1} \\
  t_{n-2} \\
  t_{n-3} \\
  \end{bmatrix} = 
    \begin{bmatrix}
  . & . & . & . & . \\
  1 & 0 & 0 & 0 & 0 \\
  0 & 1 & 0 & 0 & 0 \\
  0 & 0 & 1 & 0 & 0 \\
  0 & 0 & 0 & 1 & 0 \\
  \end{bmatrix}
  \begin{bmatrix}
  t_n \\
  t_{n-1} \\
  t_{n-2} \\
  t_{n-3} \\
  t_{n-4} \\
  \end{bmatrix}
\]

And from the coeeficients of the AR(1) model we built before, we know that $t_n$
can be expressed as a linear sum of $t_{n-1}$ through $t_{n-4}$:

\[
  \begin{bmatrix}
  t_{n+1} \\
  t_{n} \\
  t_{n-1} \\
  t_{n-2} \\
  t_{n-3} \\
  \end{bmatrix} = 
    \begin{bmatrix}
  0.508 & -0.406 & 0.348 & -0.396 & 0.246 \\
  1 & 0 & 0 & 0 & 0 \\
  0 & 1 & 0 & 0 & 0 \\
  0 & 0 & 1 & 0 & 0 \\
  0 & 0 & 0 & 1 & 0 \\
  \end{bmatrix}
  \begin{bmatrix}
  t_n \\
  t_{n-1} \\
  t_{n-2} \\
  t_{n-3} \\
  t_{n-4} \\
  \end{bmatrix}
\]

So now that, we've determined the linear operator $\boldsymbol{F}$ for 
our dynamic system, we can ask what happens to the system 2 time-steps into the future, 
then 3, and so on. It should be clear that we can simply apply $\boldsymbol{F}$ again and
again to determine *any* future state, so that in general the state at time $n$ is

\[
\boldsymbol{t}_n = \boldsymbol{F}^n \boldsymbol{t}_0
\]

But raising a matrix to a power is particularly easy if we know its eigenvalues.
Let's say $\boldsymbol{F} = \boldsymbol{Q} \boldsymbol{\Lambda} \boldsymbol{Q}^{-1}$ is
the eigen-decomposition, where $\boldsymbol{Q}$ is an orthogonal matrix and 
$\boldsymbol{\Lambda}$ is the diagonal matrix of eigenvalues. Then

\[
  \boldsymbol{F}^2 = \boldsymbol{F} \boldsymbol{F} =
  \boldsymbol{Q} \boldsymbol{\Lambda} \boldsymbol{Q}^{-1}
  \boldsymbol{Q} \boldsymbol{\Lambda} \boldsymbol{Q}^{-1}
  = \boldsymbol{Q} \boldsymbol{\Lambda}^2 \boldsymbol{Q}^{-1}
\]

This clearly generalizes to any power by induction. Also, raising a diagonal matrix to
a power is completely trivial: you simply raise each independent element to its power.

\[
\boldsymbol{\Lambda}^n = \begin{bmatrix}
  \lambda_1^n & 0 & 0 & 0 & 0 \\
  0 & \lambda_2^n & 0 & 0 & 0 \\
  0 & 0 & \lambda_3^n & 0 & 0 \\
  0 & 0 & 0 & \lambda_4^n & 0 \\
  0 & 0 & 0 & 0 & \lambda_5^n
\end{bmatrix}
\]

A few things are immediately obvious. Each eigenvalue is a complex number; so
if its norm is less than 1 it will tend to 0 as $n$ increases, or if its norm is greater than 1
it will tend to $\infty$, or if its norm is exactly 1 it will always be exactly 1. Furthermore,
if the eigenvalue is real, it will always be real, but if it is not real then it will rotate
about the origin by a fixed angle with every time step. Thus, it will exhibit some kind of oscilation
with a frequency determined by its argument. Each eigenvalue will behave independently, but if *every*
eigenvalue has norm less than 1, then the system as a whole will converge to a steady state at 0. 

So now that I've hopefully impressed upon you the importance of eigenvalues is understanding the
dynamics of our system, let's actually compute them. And, just for fun let's compare them
to the roots of the lag polynomial from above. 

```{r eigen}
ar_matrix <- matrix( nrow=5, ncol=5, byrow=TRUE, c(
  0.5078, -0.4062,   0.3481,  -0.3960,   0.2462, 
       1,       0,        0,        0,        0,
       0,       1,        0,        0,        0,
       0,       0,        1,        0,        0,
       0,       0,        0,        1,        0))

ar_eigen <- eigen(ar_matrix)
df <- t(rbind(data.frame(t(sort(1/roots))), data.frame(t(sort(ar_eigen$values)))))
colnames(df) <- c("Inverse AR(5) Roots", "Time Evolution Eigenvalues")
```

| Inverse AR(5) Roots | Time Evolution Eigenvalues |
|:-------------------:|:--------------------------:|
|-0.467 + 0.683i      | -0.467 - 0.683i            |
|-0.467 - 0.683i      | -0.467 + 0.683i            |
|0.397 + 0.630i       | 0.397 - 0.630i             |
|0.397 - 0.630i       | 0.397 + 0.630i             |
|0.649 - 0.000i       | 0.649 + 0.000i             |

Hey, wait just a minute here! What are you trying to pull here, buddy? Those are
(to within numerical precision) exactly the same as the inverse roots!

Yes, it's true. This is very obvious if we plot them together:

```{r eigenplot}
plot(
  ar_eigen$values, 
  ylim=c(-1,1), 
  xlim=c(-1,1),
  asp=1,
  cex=2,
  main="Inverse AR Roots",
  panel.first=c(
    lines(complex(modulus=1, argument=0.01*2*pi)^(0:100), col='grey'),
    abline(h=0, col='grey'),
    abline(v=0, col='grey')
  )
)

points(
  1/roots, 
  pch=4,
  cex=2,
  col='red'
)
```

They are exactly the same. You're welcome to prove this is exactly true by writing
down the characteristic polynomial for a matrix in this form and verifying its the
same polynomial we found the roots for in the AR formulation of the problem.

In fact, you can see the many parallels in the two approaches:
in one analysis, we said that an AR model would only be stationary if all
its inverse roots were inside the unit circle, in the other we said the dynamic
system would converge to a steady state at the origin. Different language, indeed
two historically different mathematical treatments, but the same conclusions. In both
cases we found that the system was characterized by a sequence of 5 complex numbers,
and that both the norm and the argument of each number meaningfully impacted the behavior
of the system. And so on.

There's no escaping it: those 5 complex numbers *are* the best way to understand this
system, and any sufficiently sophisticated approach will lead us to this same conclusion.

Let's just take a moment to realize what happened to us here: we started from a
data set entirely comprised of real numbers, built a model with real number
values for all parameters (two models in fact: a regression and matrix)
but in the end we still had to understand our
model in terms of complex numbers. The real numbers are not closed under many
interesting and natural numbers - work with real numbers long enough, and eventually
you'll be working in the complex plane. This is not infrequent and hopefully these
examples have demonstrated that its extremely helpful familiarize yourself with
R's capabilities in this regard.


[DIO]: https://en.wikipedia.org/wiki/Diophantine_equation
[EGN]: https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors
[PCA]: https://en.wikipedia.org/wiki/Principal_component_analysis
[MKV]: https://en.wikipedia.org/wiki/Markov_chain
[AR]: https://en.wikipedia.org/wiki/Autoregressive_model
[FOR]: https://cran.r-project.org/web/packages/forecast/index.html
[NSS]: https://en.wikipedia.org/wiki/Stationary_process
[UR]: https://en.wikipedia.org/wiki/Unit_root
[LDS]: https://en.wikipedia.org/wiki/Linear_dynamical_system
