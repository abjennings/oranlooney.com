---
title: 'ML From Scratch, Part 5: Gaussian Mixture Models'
author: Oran Looney
date: 2019-06-02
tags:
  - Python
  - Statistics
  - From Scratch
  - Machine Learning
image: /post/ml-from-scratch-part-5-gmm_files/lead.jpg
draft: true
---



<p>Consider the following motivating dataset:</p>
<div class="figure">
<img src="/post/ml-from-scratch-part-5-gmm_files/unlabled.png" alt="Unlabled Data" />
<p class="caption">Unlabled Data</p>
</div>
<p>It is apparent that these data have some kind of structure; which is to say, they certainly are not drawn from a uniform or other simple distribution. In particular, there is at least one cluster of data in the lower right which is clearly separate from the rest. <strong>Is it possible for a machine learning algorithm to automatically discover and model these kinds of structures without human assistance?</strong></p>
<p>It turns out that none of the techniques we have studied in this series can do this. Every model we’ve look at so far assumed that we have a clear definition of the thing that we are trying to predict **and* we already know the correct answer for every example in the training set. A problem of the form “just find me <em>some</em> kind interesting relationships or structure, any will do” does not fit into this framework because no “true” labels are known in advance.</p>
<p>More formally, all of the algorithms we’ve looked at so far have been “supervised” learning problems, where the training set consists of <em>labeled</em> pairs <span class="math inline">\((X, Y)\)</span> and the tast was to predict <span class="math inline">\(Y\)</span> from <span class="math inline">\(X\)</span>. The problem of discovering interesting structure or relationships from <em>unlabeled</em> examples <span class="math inline">\(X\)</span> is called the “unsupervised” learning problem, and calls for a different set of techniques and algorithms entirely.</p>
<div id="types-of-unsupervised-learning" class="section level2">
<h2>Types of Unsupervised Learning</h2>
<p>There are two broad approaches to unsupervised learning, which I’ll call dimensional reduction and clustering for lack of standard terminolgy.</p>
<p>The first approach is dimensional reduction, broadly speaking. In dimensional reduction we seek a function <span class="math inline">\(f : \mathbb{R}^n \mapsto \mathbb{R}^m\)</span> where <span class="math inline">\(n\)</span> is the dimension of the original data <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(m\)</span> is usually much smaller than <span class="math inline">\(n\)</span>. The classic example of a dimensional reduction algorithm is <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">PCA</a> but there are many others, including non-linear techniques like <a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">t-SNE</a>, topic models like <a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">LDA</a>, and most examples of representation learning such as <a href="https://en.wikipedia.org/wiki/Word2vec">Word2Vec</a>. The basic idea behind dimensional reduction is that by reducing to a lower dimensional space we somehow capture the essential characteristics of each data point while getting rid of noise, correlations, and other non-essential features. Furthermore, it should be possible to approximately reconstruct the original data point in the original <span class="math inline">\(n\)</span>-dimensional space with minimal loss. Depending on the specific technique used, the lower dimensional space may also be designed to have desirable properties like an isotropic/spherical covariance matrix or a meaningful distance function where data points that a human would agree are “similar” are close together. We will return to dimensional reduction in some future article.</p>
<p>The second approach to unsupervised learning is generally called clustering and is charactering by seeking a function <span class="math inline">\(f : \mathbb{R}^n \mapsto \{1,2, ..., k\}\)</span> which maps each example to exactly one of <span class="math inline">\(k\)</span> possible classes. The classic example of a clustering algorithm is <a href="https://en.wikipedia.org/wiki/K-means_clustering"><span class="math inline">\(k\)</span>-means</a>. Reducing rich, multivariate data to a small finite number of possibilities seems extreme, but for that same reason it can be extremely clarifying as well. In this article we will implement on particular clustering model called the <a href="https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model">Gaussian mixture model</a>, or just GMM for short.</p>
</div>
<div id="gaussian-mixture-models" class="section level2">
<h2>Gaussian Mixture Models</h2>
<p>The Gaussian mixture model is simply a “mix” of Gaussian distributions. In this case, “Gaussian” means the <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">multivariate normal distribution</a> <span class="math inline">\(\mathcal{N}(\mathbf{\mu}, \Sigma)\)</span> and “mixture” means that the several different gaussian distributions, all with different mean vectors <span class="math inline">\(\boldsymbol{\mu}_j\)</span> and different covariance matrices <span class="math inline">\(\Sigma_j\)</span>, are combined by taking the weighted sum of the probability density functions:</p>
<p><span class="math display">\[ \begin{align}
    f_{GMM}(\mathbf{x}) = \sum^k_{j=1} \phi_j f_{\mathcal{N}(\boldsymbol{\mu}_j, \Sigma_j)}(\mathbf{x}) \tag{1}
   \end{align}
\]</span></p>
<p>subject to:</p>
<p><span class="math display">\[
  \sum_{j=1}^k \phi_j = 1 \tag{2}
\]</span></p>
<p>A single multivariate normal distribution has a single “hill” or “bump” located at <span class="math inline">\(\mu_i\)</span>; in contrast, a GMM (in general) is a multimodal distribution with on distinct bump per class. (Sometimes you get fewer than <span class="math inline">\(k\)</span> distinct local maxima in the p.d.f., if the bumps are sufficiently close together or if the weight of one class is zero or nearly so, but usually you will get <span class="math inline">\(k\)</span> distinct bumps.) This makes it well suited to modeling data like that seen in our motivating example above, where there seems to be more than one region on high density.</p>
<p><a href="https://mathematica.stackexchange.com/questions/15055/finding-distribution-parameters-of-a-gaussian-mixture-distribution"><img src="/post/ml-from-scratch-part-5-gmm_files/gmm_pdf.png" alt="GMM p.d.f." /></a></p>
<p>We can view this is as a two-step generative process. To generate the <span class="math inline">\(i\)</span>-th example:</p>
<ol style="list-style-type: decimal">
<li>Sample a random class index <span class="math inline">\(C_i\)</span> from the categorical distribution parameterized by <span class="math inline">\(\boldsymbol{\phi} = (\phi_1, ... \phi_k)\)</span>.</li>
<li>Sample a random vector <span class="math inline">\(\mathbf{X}_i\)</span> from the multivariate distribution associated to the <span class="math inline">\(C_i\)</span>-th class.</li>
</ol>
<p>The <span class="math inline">\(n\)</span> independent samples <span class="math inline">\(\mathbf{X}_i\)</span> are the row vectors of the matrix <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p>Symbolically, we write:</p>
<p><span class="math display">\[ \begin{align}
    C_i &amp; \sim \text{Categorical}(k, \boldsymbol{\phi}) \tag{3} \\
    \mathbf{X}_i &amp; \sim \mathcal{N}(\boldsymbol{\mu}_{C_i}, \Sigma_{C_i}) \tag{4} \\
   \end{align}
\]</span></p>
<p>To fit a GMM model to a particular dataset, we attempt to find the <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximum likelihood estimate</a> of the parameters:</p>
<p><span class="math display">\[\Theta = \{ \mathbf{\mu}_1, \Sigma_1, ..., \mathbf{\mu}_k, \Sigma_k \} \tag{5} \]</span></p>
<p>Because the <span class="math inline">\(n \times m\)</span> example matrix <span class="math inline">\(\mathbf{X}\)</span> is assumed to be a realization of <span class="math inline">\(n\)</span> i.i.d. samples from <span class="math inline">\(f_{GMM}(\mathbf{x})\)</span>, we can write down our likelihood function as</p>
<p><span class="math display">\[
  \mathcal{L}(\Theta; \mathbf{X}) = P(\mathbf{X}_i;\Theta) = \prod_{i=1}^n \sum_{j=1}^k P(C_i=j|\mathbf{X}_i)P(\mathbf{X}_i|C_i=j) \tag{6}
\]</span></p>
<p>We know that <span class="math inline">\(\mathbf{X}_i\)</span> has a multivariate normal distribution with parameters determined by the class, so the second condifitional probability can be written down pretty much immediately from the definition:</p>
<p><span class="math display">\[
  P(\mathbf{X}_i|C_i=j) = \frac{1}{\sqrt{(2\pi)^k |\Sigma_j|}} \text{exp}\Bigg( 
    - \frac{(\mathbf{X}_i - \boldsymbol{\mu}_j)^T \Sigma_j^{-1} (\mathbf{X}_i - \boldsymbol{\mu}_j) }
         {2} 
  \Bigg) \tag{7}
\]</span></p>
<p>The second part needs a little more work. We know that the unconditional probability is given by the parameter <span class="math inline">\(\boldsymbol{\phi}\)</span>:</p>
<p><span class="math display">\[
P(C_i = j) = \phi_j \tag{8}
\]</span></p>
<p>So using Bayes’ theorem, we can write this in terms of equation (7):</p>
<p><span class="math display">\[
\begin{align}
P(C_i=j|\mathbf{X}_i) 
  &amp; = \frac{P(C_i=j) P(\mathbf{X}_i|C_i=j)}
  {P(\mathbf{X}_i)} \\
  &amp; = \frac{ \phi_j P(\mathbf{X}_i|C_i=j)}
  {\sum_{l=1}^k P(\mathbf{X}_i|C_i=l)} \\
\end{align} \tag{9}
\]</span></p>
<p>If we substituted equation (7) into (9) we could get a more explicit but very ugly formula, so I leave that to the reader’s imagination.</p>
<p>Equations (6), (7), and (9), when taken together, constitute the complete likelihood function <span class="math inline">\(\mathcal{L}(\Theta;\mathbf{X})\)</span>, yet the problem of how to find the MLE estimate seems as far from a solution as ever because this equation seems rather intractable. Certainly I would not care to write it out in fully explicit form and then try to take partial derivatives with represent to <span class="math inline">\(\boldsymbol{\phi}\)</span>, <span class="math inline">\(\boldsymbol{\mu}_j\)</span>, and <span class="math inline">\(\Sigma_j\)</span>! Luckily, we don’t have to. The form we have it in now suffices to allow us to write down an iterative algorithm which will solve the MLE problem for us.</p>
</div>
<div id="the-em-algorithm" class="section level2">
<h2>The EM Algorithm</h2>
<p>TODO. Very broad and powerful algorithm. Also used for Factor Analysis and the <a href="https://courses.cs.washington.edu/courses/cse590q/04au/papers/WinklerEM.pdf">Fellegi-Sunter</a> record linkage algorithm, and many other “latent variable” models.</p>
<p>Coordinate descent, every iteration improves, we end with a ML estimate.</p>
<p>One good resource on GMM and the EM algorithm I used was this <a href="https://youtu.be/ZZGTuAkF-Hw?t=2108">Stanford lecture by Andrew Ng</a>. I’ve linked to the part of the lecture where he shows this update step in particular but the whole lecture is worth watching. Another good resource is this <a href="http://www.cs.cmu.edu/~guestrin/Class/10701-S07/Slides/em-baumwelch.pdf">slide deck</a>; I found that working through the simple, finite exercise by hand to be a great way to build intuition before tackling the much more complicated GMM case.</p>
<p>Animation:</p>
<p><a title="Chire [CC BY-SA 3.0 (https://creativecommons.org/licenses/by-sa/3.0)], via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File:EM_Clustering_of_Old_Faithful_data.gif"><img width="256" alt="EM Clustering of Old Faithful data" src="https://upload.wikimedia.org/wikipedia/commons/6/69/EM_Clustering_of_Old_Faithful_data.gif"></a></p>
</div>
<div id="understanding-the-e-step" class="section level2">
<h2>Understanding the E-step</h2>
<p>Given the that centroid <span class="math inline">\(\mu\)</span> and covariance matrix <span class="math inline">\(\Sigma\)</span> is known for each class we can update <span class="math inline">\(w_{ij}\)</span> by simply calculating the probability that <span class="math inline">\(X_i\)</span> came from each class and normalizing TODO:</p>
<p><span class="math display">\[ w_{ij} = \frac{ P(X_i|K=j) }{ P(K_i) } = \frac{ P(X_i|K=j) }{ \sum_{l=1}^k P(X_i|K=l) }
\]</span></p>
<p>The conditional probablity <span class="math inline">\(P(\mathbf{X}_i|K=j)\)</span> is simply the multivariate normal distribution <span class="math inline">\(\mathbf{X}_i ~ \mathcal{N}(\mu_i, \Sigma_i)\)</span> so we can use equation (4) above to calculate the probability density for each class, and then divide through by the total to normalize each row of <span class="math inline">\(\mathbf{X}\)</span> to 1. This gives us a concrete formula for the update to <span class="math inline">\(w_ij\)</span>:</p>
<p><span class="math display">\[ w_{ij} := \frac{ f_{\mathcal{N}(\mu_i, \Sigma_i)}(\mathbf{X}_i) }
                 { \sum_{l=1}^k f_{\mathcal{N}(\mu_l, \Sigma_l)}(\mathbf{X}_i) }
\]</span></p>
<p>The probability of each class <span class="math inline">\(\phi\)</span> can then be estimated by averaging over all examples in the training set:</p>
<p><span class="math display">\[ \phi_j = \sum_{i=1}^n w_{ij} \]</span></p>
</div>
<div id="understanding-the-m-step" class="section level2">
<h2>Understanding the M-step</h2>
<p>Yes, and it’s easy to see why if we look at it the right way.</p>
<p>Forget about the M-step; in fact, forget the fact that we’re inside an iterative algorithm at all. Forget about the past and that you ever had other estimates for <span class="math inline">\(\vec{\mu}\)</span> or <span class="math inline">\(\mathbb{\Sigma}\)</span>. And forget that there are other classes, and just focus on one class. And forget that the weights can be interpreted as <span class="math inline">\(p(m|\vec{x}_i;\theta)\)</span> and just think of them as sample weights, origin unknown, called <span class="math inline">\(w_i\)</span>. For convenience let’s also assume that these weights are normalized, e.g. <span class="math inline">\(\sum w_i = 1\)</span>.</p>
<p>What remains is a sample of <span class="math inline">\(n\)</span> (instead of <span class="math inline">\(T\)</span> so I can reserve <span class="math inline">\(T\)</span> for “transpose”) observations <span class="math inline">\(\vec{x}_i\)</span> with weights <span class="math inline">\(w_i\)</span> which we believe came from a multivariate distribution <span class="math inline">\(\mathcal{N}(\vec{\mu}, \mathbb{\Sigma})\)</span>. What is the <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Estimation_of_parameters">procedure for estimating</a> <span class="math inline">\(\vec{\mu}\)</span> and <span class="math inline">\(\mathbb{\Sigma}\)</span>? Well, it’s to first calculate the mean, subtract it off, then average over the outer products:</p>
<p><span class="math display">\[ \vec{\mu}= {1 \over {n}}\sum_{i=1}^n w_i \vec{x}_i \tag{1} \]</span></p>
<p><span class="math display">\[ \mathbb{\Sigma} = \frac{1}{n} \sum_{i=1}^n w_i ( \vec{x}_i - \vec{\mu} )(\vec{x}_i - \vec{\mu})^T \tag{2} \]</span></p>
<p>This is the <a href="https://www.cs.cmu.edu/~epxing/Class/10701-08s/recitation/gaussian.pdf">ML estimate</a>. Note that the <span class="math inline">\(\vec{\mu}\)</span> in (2) is exactly the same as the <span class="math inline">\(\vec{\mu}\)</span> in (1) - this is a closed form solution, not an iterative algorithm! Any <em>other</em> estimate for <span class="math inline">\(\vec{\mu}\)</span> and <span class="math inline">\(\mathbb{\Sigma}\)</span> will have lower likelihood, by definition.</p>
<p>Now we can lift our self imposed amnesia and recall that we are using E-M to numerically approximate the maximum likelihood of a GMM. For that one particular M step, we held all other parameters constant and maximized the likelihood with respect to just <span class="math inline">\(\vec{\mu}\)</span> and <span class="math inline">\(\mathbb{\Sigma}\)</span>. Because we used the closed form solution of (1) and (2), we didn’t take a “step” <em>towards</em> that maximum - we jumped straight to the maximum within that subspace. In particular we can guarantee that likelihood either increased or stayed the same. We need this property because it is necessary for the convergence of the E-M algorithm as a whole. Using the closed form ML estimate for the multivariate distribution is by easiest way to prove it, and any other procedure for updating <span class="math inline">\(\vec{\mu}\)</span> and <span class="math inline">\(\mathbb{\Sigma}\)</span> - such as using the <span class="math inline">\(\mu\)</span> left over from the previous step - would require a separate proof.</p>
</div>
<div id="implementation" class="section level2">
<h2>Implementation</h2>
<pre><code>import numpy as np
from scipy.stats import multivariate_normal

class GMM:
    def __init__(self, k, max_iter=5):
        self.k = k
        self.max_iter = int(max_iter)

    def initialize(self, X):
        self.shape = X.shape
        self.n, self.m = self.shape

        self.phi = np.full(shape=self.k, fill_value=1/self.k)
        self.weights = np.full( shape=self.shape, fill_value=1/self.k)
        
        random_row = np.random.randint(low=0, high=self.n, size=self.k)
        self.mu = [  X[row_index,:] for row_index in random_row ]
        self.sigma = [ np.cov(X.T) for _ in range(self.k) ]

    def e_step(self, X):
        # E-Step: update weights and phi holding mu and sigma constant
        self.weights = self.predict_proba(X)
        self.phi = self.weights.mean(axis=0)
    
    def m_step(self, X):
        # M-Step: update mu and sigma holding phi and weights constant
        for i in range(self.k):
            weight = self.weights[:, [i]]
            total_weight = weight.sum()
            self.mu[i] = (X * weight).sum(axis=0) / total_weight
            self.sigma[i] = np.cov(X.T, 
                aweights=(weight/total_weight).flatten(), 
                bias=True)

    def fit(self, X):
        self.initialize(X)
        
        for iteration in range(self.max_iter):
            self.e_step(X)
            self.m_step(X)
            
    def predict_proba(self, X):
        likelihood = np.zeros( (self.n, self.k) )
        for i in range(self.k):
            distribution = multivariate_normal(
                mean=self.mu[i], 
                cov=self.sigma[i])
            likelihood[:,i] = distribution.pdf(X)
        
        numerator = likelihood * self.phi
        denominator = numerator.sum(axis=1)[:, np.newaxis]
        weights = numerator / denominator
        return weights
    
    def predict(self, X):
        weights = self.predict_proba(X)
        return np.argmax(weights, axis=1)</code></pre>
</div>
<div id="model-evaluation" class="section level2">
<h2>Model Evaluation</h2>
<p>We’ll use the “iris” dataset. This dataset has labels, but we won’t expose them to the model, but we will use them at the end to discuss the question, “were we able to discover the class labels through unsupervised learning?”</p>
<pre><code>from scipy.stats import mode
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
iris = load_iris()
X = iris.data</code></pre>
<p>Fit a model.</p>
<pre><code>np.random.seed(42)
gmm = GMM(k=3, max_iter=10)
gmm.fit(X)</code></pre>
<p>Plot. Each color is a cluster found by GMM:</p>
<pre><code>def jitter(x):
    return x + np.random.uniform(low=-0.05, high=0.05, size=x.shape)

def plot_axis_pairs(X, axis_pairs, clusters, classes):
    n_rows = len(axis_pairs) // 2
    n_cols = 2
    plt.figure(figsize=(16, 10))
    for index, (x_axis, y_axis) in enumerate(axis_pairs):
        plt.subplot(n_rows, n_cols, index+1)
        plt.title(&#39;GMM Clusters&#39;)
        plt.xlabel(iris.feature_names[x_axis])
        plt.ylabel(iris.feature_names[y_axis])
        plt.scatter(
            jitter(X[:, x_axis]), 
            jitter(X[:, y_axis]), 
            #c=clusters, 
            cmap=plt.cm.get_cmap(&#39;brg&#39;),
            marker=&#39;x&#39;)
    plt.tight_layout()
    
plot_axis_pairs(
    X=X,
    axis_pairs=[ 
        (0,1), (2,3), 
        (0,2), (1,3) ],
    clusters=permuted_prediction,
    classes=iris.target)</code></pre>
<div class="figure">
<img src="/post/ml-from-scratch-part-5-gmm_files/gmm_clusters.png" alt="GMM Clusters" />
<p class="caption">GMM Clusters</p>
</div>
<p>Well, the model certainly found <em>something.</em></p>
</div>
<div id="comparing-to-true-class-labels" class="section level2">
<h2>Comparing to True Class Labels</h2>
<p>Are the clusters discovered by the GMM model <em>meaningful</em>? Are they <em>correct</em>? For a real-world unsupervised learning problem, these questions simply have no answers. However, it so happens that the iris dataset we used <em>is</em> actually labeled. True, we didn’t make use of these labels when training the GMM model, but they are available. Futhermore, those classes <em>are</em> assocated with different distributions in the 4 observed variables in a way that closely matches the assumptions of the GMM. So even if we can’t ask about “meaning” and “correctness”, we can at least ask a closely related question: “did this unsupervised learning algorithm (re-)discover the known structure of the data set?”</p>
<p>The cluster indexes found by the model are in random order. For convenience when comparing them to true class labels, we will permute them to be as similar as possible to true class labels. All this is doing is swapping 0 for 2 so that 0 means the same thing for both the clusters and for the original class labels.</p>
<pre><code>permutation = np.array([
    mode(iris.target[gmm.predict(X) == i]).mode.item() 
    for i in range(gmm.k)])
permuted_prediction = permutation[gmm.predict(X)]
print(np.mean(iris.target == permuted_prediction))
confusion_matrix(iris.target, permuted_prediction)


0.96
array([[50,  0,  0],
       [ 0, 44,  6],
       [ 0,  0, 50]])</code></pre>
<p>After 1,000 random trials, we can see that cluster-to-labels “accuracy” actually varies at random from 0.52 to 0.99 with a mean of 0.74.</p>
<div class="figure">
<img src="/post/ml-from-scratch-part-5-gmm_files/accuracy_histogram.png" alt="Accuracy Histogram" />
<p class="caption">Accuracy Histogram</p>
</div>
</div>
<div id="limitations" class="section level2">
<h2>Limitations</h2>
<p>All unsupervised learning methods known today share certain limitations.</p>
<p>First, they tend to rely on the research fixing certain arbitary complexity parameters such as the number of dimensions <span class="math inline">\(m\)</span> or the number of clusters <span class="math inline">\(k\)</span>. Worse still, while there are techniques for picking these complexity parameters, they are heuristic and often unsatisfying in practice. It can be very hard to tell if an unsupervised learning method is “overfitting”, because “overfit” doesn’t even have a precise definition for unsupervised learning problems.</p>
<p>Second, there are no hard metrics like accuracy or AUC that let you compare models from different families. While each unsupervised learning algorithm will have its own internal metrics which they try to optimize such as variance explained or perplexity, these usually can’t be meaningful compare two models that use two different algorithms or with different complexity parameters. This makes model selection a fundamentally subjective task - to decide that t-SNE is doing a better job than k-means on a given data set, the modeler is often reduced to eyeballing the output.</p>
<p>Third and finally, the factors and/or clusters discovered by unsupervised learning algorithms are often unsatisfying or counterintuitive and don’t necessarily line up with human intuition. Another way of saying the same thing is that if a human goes through and creates labels <span class="math inline">\(\mathbf{Y}\)</span> for the training set <span class="math inline">\(\mathbf{X}\)</span> after the unsupervised learning algorithm has been applied to it, they are not very likely to come up with the same factors or clusters. In general, humans tend to come up with business rules that “make sense” but don’t explain as much variance as possible, while algorithms tend to find “deep” features that do explain a lot of variance but have complicated definitions that are hard to wrap your head around.</p>
<p>These seem like serious criticisms; does this mean we shouldn’t use unsupervised learning? Well, I won’t tell you that you categorically should not use it, but you should know what you’re getting into. By default, it tends to produce low-quality, hard-to-interpret models that cannot really be defended due to number of subjective decisions needed to make them work at all.</p>
<p>On the other hand, unsupervised learning can be extremely helpful during exploratory research; also, in the form of representation learning, it can sometimes accelerate learning or improve performance, or allow models to generalize from an extremely limited labeled training set. For example, a sentiment analysis model trained on only a few hundred reviews may only see the word “sterling” once, but if it uses a word embedding model like word2vec, it will understand that “stupenduous” is broadly a synonym for “good” or “great”, and will therefore by abling to correctly classify a future example with the word “stupenduous” - which did not appear even once in the training set - as likely having positive sentiment. While success stories like this are possible, in general unsupervised learning requires more expertise, more manual tuning, and more input from domain experts in order to create value. Unfortunately, we do not always have the labels necessary for supervised learning, and the datasets available may be too large, too high dimensional, or too sparse to be ammenible to traditional techniques; it is in these situations where the benefits of unsupervised learning can outweigh the negatives.</p>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>In this article, we seen how unsupervised learning differs from supervised learning and the challenges that come along with that. We discussed a method for posing an unsupervised learning problem as an maximimum likelihood optimization, and described and implemented the <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">EM algorithm</a> often used to solve these otherwise intractable problems. We’ve seen first hand that the clusters identified by such algorithms don’t always line up perfectly with what we believe the true structure to be.</p>
<p>TODO: my bias against them and other warnings.</p>
</div>
