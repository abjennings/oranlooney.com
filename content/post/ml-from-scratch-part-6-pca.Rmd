---
title: 'ML From Scratch, Part 6: Principal Component Analysis'
author: Oran Looney
date: 2019-08-01
tags:
  - Python
  - Statistics
  - From Scratch
  - Machine Learning
image: /post/ml-from-scratch-part-6-pca_files/lead.jpg
draft: true
---

In the [last article][MLFS5] in [this series][MLFS], we distinguished between two
kinds of unsupervised learning: cluster analysis and dimensionality reduction. 
Only the first was discussed in detail at that time, so in this article we will
turn our attention to the later.

In dimensional reduction we seek a function $f : \mathbb{R}^n \mapsto
\mathbb{R}^m$ where $n$ is the dimension of the original data $\mathbf{X}$ and
$m$ is smaller than $n$. That is, want a map from some high dimensional space
into some lower dimensional space. In this article, we will focus on the oldest
and perhaps simplest of these methods: [Primary Component Analysis][PCA], usually
seen abbreviated as PCA.

In this article, we'll derive PCA from first principles, implement a working
version (writing all the linear algebra from scratch), discuss options for choosing how
many dimensions to keep, and show an example of how PCA helps us visualize
and gain insight into a 13-dimensional dataset.

What is PCA?
------------

First of all, PCA is a *linear* dimensionality reduction technique; some other
dimensionality techniques, such as [t-SNE][TSNE], allow this map to be
non-linear, but we will restrict ourselves to linear maps for now. Just saying
it's linear, though, is not quite enough to fully specify the problem; [Factor
Analysis][FA] also seeks a linear map, but takes quite a different theoretical
approach and reaches a slightly different solution in practice.

There are two ways we can further constrain the problem to arrive at PCA.

1. Require the covariance matrix of the transformed data to be diagonal. This
   is equivalent to saying that the transformed data has no [multicollinearity][MC],
   or that all $m$ features of the transformed data are uncorrelated.
2. Seek a new basis for the data such that the first basis vector points
   in the direction of maximum variation, or in other words is the "principle
   component" of our data. Then require that the second basis vector points
   also points in the direction of maximum variation in the plane orthogonal
   to the first, and so on until a new orthonormal basis is constructed.

These two definitions turn out to be equivalent. Both construct a unique
(\*some terms and conditions apply) orthogonal matrix $Q$ which can be matrix
multiplied by the original data matrix $X$ to obtain the transformed data $X' =
XQ$.  (Read "X prime". I never use the prime mark for transpose, and will
always use superscript "T" like $X^T$ for transpose.) 

Orthogonal matrices are the generalization of the 3-dimensional concept of a
rotation or reflection: in particular, they always preserve both *distance* and
*angles*.  These are very properties for a transform to have! Merely rotating
an object doesn't really distort it, but simply gives us a different
perspective on it. In this sense, PCA is the *least destructive* transformation
that we could apply. 

Now, obviously we could rotate our data any which way to get a different
picture, but we want to rotate it so that in some sense in becomes aligned to
the axes - rather like straightening a picture hanging askew on the wall.  Near
the end, we'll show examples demonstrating how this "straightening up" helps
with analysis and interpretation.

Now, orthogonal matrices are invertible, which in particular means they are
square, so $Q$ is an $m \times m$ matrix... which means we haven't *reduced*
the dimensionality at all! Why is this considered a dimensionality *reduction*
technique? Well, it turns out that once we've rotated our data so that it's as
*wide* as possible along the first basis vector, that also means that it ends
up as *thin* as possible along the last few basis vectors. This only works if
the original data really were all quite close to some line or hyperplane, but
when it does it means we can safely drop the some dimensions and retain only
our principle components, thus reducing dimensionality while still keeping most
of the information (variance) of the data.  Of course, deciding *how many*
dimensions to drop and how many to keep is a bit tricky, but we'll come to that
later.

For now, let's explore the mathematics and show how PCA gives rise to a unique
solution subject to the above constraints.

The Direction of Maximal Variation
----------------------------------

Before we can define the direction of maximal variance, we first have to be
clear about what we mean by variance in a given direction. First, let's say
that $\mathbf{x}$ is an $n$-dimensional random vector. This represents the
population our data will be sampled from. Next, suppose you have some
non-random vector $\mathbf{q} \in \mathbb{R}^n$. Assuming this vector is
non-zero, it defines a line. What do mean by the phrase, "the variance of
$\mathbf{x}$ in the direction of $\mathbf{q}$?" 

The natural thing to do is to *project* the $n$-dimensional random variable
*onto* the line defined by $\mathbf{q}$. We can do this with a dot product,
$\mathbf{q}^T \mathbf{x}$. This new quantity is clearly a scalar random variable,
so we can apply the variance operator to get a scalar measure of variance.

$$ \operatorname{Var}[ \mathbf{q}^T \mathbf{x} ] \tag{1} $$

Does this suffice to allow us to define a direction of maximal variation? Not
quite. If we try to pose the optimization problem:

$$ \underset{\mathbf{q}}{\operatorname{argmax}} \operatorname{Var}[ \mathbf{q}^T \mathbf{x} ] \tag{2} $$

To prove no such maximum exists assume that $\mathbf{q}$ is the maximum. There
always exists another vector $\mathbf{r} = 2 \mathbf{q}$ which implies that:

$$\operatorname{Var}[ \mathbf{r}^T \mathbf{x}]  = 4 \operatorname{Var}[ \mathbf{q}^T \mathbf{x} ] > \operatorname{Var}[ \mathbf{q}^T \mathbf{x} ] \tag{3}$$.

Which implies that $\mathbf{q}$ was not the maximum after all.  So we cannot
solve this optimization problem unless we impose some additional constraint.

Now, a dot product is only a projection in a geometric sense if $\mathbf{q}$ is
a *unit* vector. So why don't we impose the condition

$$ \mathbf{q}^T \mathbf{q} = 1 \tag{4} $$

That gives us the constrained optimization problem 

$$ \underset{\mathbf{q}}{\operatorname{argmax}} \operatorname{Var}[ \mathbf{q}^T \mathbf{x} ] \quad\quad \text{such that} \, \mathbf{q}^T \mathbf{q} = 1 \tag{5} $$

Well at least this *has* a solution, even if it isn't immediately obvious how
to solve it. We can't simply set partial derivative with respect to
$\mathbf{q}$ equal to zero; that KKT condition only applies in the *absence* of
active constraints.  Earlier in this series, we've used techniques such as
stochastic gradient descent to solve unconstrained optimization problems, but
how do we deal the constraint?

A very general and powerful solution is the method of Lagrange multipliers.
Lagrange was studying the motion of constrained physical systems such as a bead
on a wire or several pendulums coupled together.  At the time, such problems
were usually solved by finding a suitable set of generalized coordinates, but
this required a great deal of ingenuity and (as we say today) didn't really
scale. Langrange's solution was to imagine the system could "slip" just ever so
slightly out of its constraints but that the true solution would be the one
that *minimized* this virtual slipage. This could be elegantly handled by
associating an energy cost called 'virtual work" that penalized the system
proportional to the degree to which the constraints were violated. This
reconceptualizes the hard constraint as just another parameter to optimize in
an unconstrained system! And surprisingly enough, it does not result in an
*approximate* solution that only sort of obeys the constraint but instead
(assuming the constraint is physically possible and the resulting equations
have a closed form solution) gives an *exact* solution where the constraint is
*perfectly* obeyed! 

It's easy to use too, at least in our simple case. We introduce the Lagrange
multiplier $\lambda$ and rewrite our optimization as follows:

$$ \underset{\mathbf{q} ,\, \lambda}{\operatorname{argmax}} \operatorname{Var}[ \mathbf{q}^T \mathbf{x} ] + \lambda (\mathbf{q}^T \mathbf{q} -1) \tag{6} $$

Why is this the same as the above? Let's call the above function $f(\mathbf{q}, \lambda)$ write down the KKT conditions:

$$ 
  \begin{align}
	\nabla_\mathbf{q} f & = 0 \tag{7} \\
	\frac{\partial f}{\partial \lambda} & = 0 \tag{8}
  \end{align}
$$

But equation (8) is simply $\mathbf{q}^T \mathbf{q} - 1 = 0$ is simply our unit
vector constraint... this guarantees that when we solve (7) and (8), the
constraint will be exactly satisfied and we'll will also have found a solution
to (6).  Such is the magic of Lagrange multipliers.

But if (8) is just our constraint in a fancy new dress, how have we progressed at all?
Because (7) is now unconstrained and therefore more tractable.

TODO: introduce a realization $X$. 









Diagonalizing the Covariance Matrix
-----------------------------------


Implementation
--------------


Wine Quality Example
--------------------


Strategies for Choosing The Number of Dimensions
------------------------------------------------


Conclusion
----------



[MLFS5]: /post/ml-from-scratch-part-5-gmm/
[MLFS]: /tags/from-scratch/
[PCA]: https://en.wikipedia.org/wiki/Principal_component_analysis
[TSNE]: https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding
[W2V]: https://en.wikipedia.org/wiki/Word2vec
[LDA]: https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation
[FA]: https://en.wikipedia.org/wiki/Factor_analysis
[MC]: https://en.wikipedia.org/wiki/Multicollinearity
