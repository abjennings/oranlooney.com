---
title: 'ML From Scratch, Part 1: Linear Regression'
author: Oran Looney
date: '2018-11-20'
slug: ml-from-scratch-part-1-linear-regression
tags:
  - Python
  - Statistics
  - From Scratch
  - Machine Learning
image: /post/ml-from-scratch-part-1-linear-regression_files/lead.png
draft: true
---

To kick off this series, will start with something simple yet foundational:
linear regression via [ordinary least squares][OLS]. 

While not exciting, linear regression finds widespread use both as a standalone
learning algorithm and as a building block in more complicated learning
algorithms: the output layer of a deep neural network trained for regression
with MSE loss, the "gradient boosting" used by gradient boosting trees, and the
"local regression" part of LOWESS smoothing are all (modified) examples of
linear regression.

Linear regression is also the "simple harmonic oscillator" of machine learning;
that is to say, a chance to apply our theoretical ideas about machine learning
to a case that is not mathematically taxing.

There is also the small matter of it being the most widely used statistical
learning algorithm in the world - but because it is mainly used by people who
call themselves "statisticians," "analysts," or "researchers" instead of "data
scientists", we can safely ignore that consideration; don't you think?

However, since I can feel your eyes glazing over from such an introductory
topic, we can spice things up a little bit by doing something which isn't often
done in introductory machine learning - we can present the algorithm that
[insert your favorite statistical software package here] uses to *actually* fit
linear regression models: [QR decomposition][QRD]. This is commonly glossed
over because it involves a lot of linear algebra and doesn't generalize well to
other machine learning algorithms so most teachers prefer to teach gradient
descent here so they can build on it in later lectures - but that's not how
linear models are fit in the real world. Besides, the linear algebra involved
is actually exceptionally beautiful and interesting.

Statistics
----------

Let's start by posing the problem and deriving the so-called [normal equation.][NEQ] Let's say that $X_p$
is a random vector of length $M$ and $y_p$ is a scalar random variable. $X_p$ and $y_p$ are *not* independent,
but have a joint probability distribution $F(x, y; \Theta, \sigma)$ parameterized by a 
non-random parameter vector $\Theta_p$, a non-negative scalar $\sigma$, and a random error term $\epsilon \sim \mathcal{N}(0, \sigma^2)$.

\[ y_p = X_p \Theta_p + \epsilon \]

Then suppose we sample $N$ observations from this joint distribution. We place the $N$ observations
into a real-valued $N\times k$ matrix $X$ and a real-valued vector $y$. Just to be absolutely clear,
$X$ and $y$ are *not* random variables - they are the given data set we use to fit the model. We
can then ask, what is the likelihood of obtaining the realization $(X, y)$ from a parameter vector $\Theta$?
By rearranging our equation as $y_p - X_p\Theta = \epsilon ~ \mathcal{N}(0, \sigma^2)$ and using
the p.d.f. of the normal distribution, we can see that:

\[ L(\Theta|X,y) = \frac{1}{\sqrt{2\pi}\sigma} \prod_{i=1}^{N}\text{exp}\Big(\frac{-(y_i - X_i\Theta)^2}{2\sigma^2} \Big) \]

That looks pretty awful, but there are a couple easy things we can do to make it a look a lot simplier. First,
That constant term out front doesn't matter at all. We can also take that $e^{-2\sigma^2}$ outside the product
as $e^{-2N\sigma^2}$, and which we'll also stuff in the constant $C$ because we're only interested in $\Theta$ right now. Finally, we can take a log to get rid of the exponential and turn the product into a sum. All together, we
get the log-likelihood expression:

\[ \log L(\Theta|X,y) = C - \sum_{i=1}^N -(y_i - X_i\Theta)^2 = C - ||y - X\Theta||^2 \]

Now, because log is a monotonically increasing function, maximizing $l$ is the same as maximizing $L$. And of course
any constant $C$ has no effect whatsoever on the location of the maximum. We can also remove the minus sign and
consider the problem as a minimization problem instead. Therefore
our [maximum likelihood estimate][MLE] of $\Theta$ for a given data set $(X, y)$ is simply:

\[ \hat{\Theta} \triangleq \underset{\Theta}{\text{argmin}} \, ||y - X\Theta||^2 \]

If your eyes were glazing over from all the statistical nomenclature, I have some good news for you: we're
done with the statistics. Everything in this final equation is now a real-valued vector or matrix; there's not a
random variable or probability distribution in sight! It's all over except for the linear algebra.

What we did above was essentially a short sketch of the relevant parts of the
[Gauss-Markov theorem][GMT] which proves that the OLS solution *is* the MLE for
the particular statistical model we started with. The full Gauss-Markov theorem also
proves a bunch of other nice properties that we won't concern ourselves with
here. 

If you remember the motivating example I gave in [Part 0][ML0], it wasn't
obvious at the time that it was even *possible* to come up with parameters for
a model in a systematic way. Now we know that not only is it always *possible*,
but there's actually at least one concrete algorithm which solves the problem
in polynomial time and requires only a small amount of data (usually some small
multiple of the number of parameters to fit.) 

Our key takeaway is that if it's true that our response variable is
related to our predictor variables by a linear equation plus a certain amount
of random Gaussian noise, then we can recover good, unbiased estimates of that
linear equations coefficients from nothing more than a finite number of data
points sampled from the underlying distribution, and the way to actually
calculate those estimates is to solve the OLS problem for the data set.

[ML0]: /post/ml-from-scratch-part-0-introduction/

Ordinary Least Squares
----------------------

Note: for this next section, we're going to be doing some light vector calculus. I suggest
you reference [the matrix cookbook][TMC] if any of the notation or concepts aren't familiar.

[TMC]: https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf

Let's call the right-hand side (the part we're trying to minimize) $J$. Then we have:

\[ J = || y - X\Theta ||^2 \]

And the problem is to minimize J with respect to $\Theta$. As optimization
problems go, this one is pretty easy: it's continuous, quadratic, convex,
everywhere differentiable, and unconstrained. 
Because of these nice properties, we know that it has a unique global minimum,
and that the gradient is zero at this minimum.
That's a fancy way of saying that
it's shaped like a big, round bowl so it's obvious where the bottom is.

\[ \begin{split} \nabla_\Theta J & = \nabla_\Theta \; (y - X\Theta)^T(y - X\Theta) \\
& = \nabla_\Theta \; y^T y - (X\Theta)^T y - y^T X\Theta + \Theta^T (X^T X) \Theta \end{split}\]
   
It's obvious that $y^T y$ is constant with respect to $\Theta$ so the first term simply vanishes. 
It's less obvious but also true that the next two terms are equal to each other - 
remember that a J is a scalar,
so those terms are each scalar, and the transpose of a scalar is itself. The final term is a 
quadratic form, and the [general rule][QFG] is $ q \nabla x^T A x = \frac{1}{2}A^T x + \frac{1}{2}A x $
but because the product of a matrix with itself is always symmetric ($X^T X = (X^T X)^T$) we can use
the simpler form $\nabla x^T A x = 2 A x$.

\[ \nabla_\Theta J = - 2 y^T X + 2 X^T X \Theta \]
Setting this equal to zero at $\hat{\Theta}$ we get:

\[ (X^T X) \hat{\Theta} = y^T X \]

The right hand side is a known vector, the left-hand side is a matrix times an unknown vector, so this
is just the familiar equation for solving for a particular solution to a system of equations $Ax = b$.

Because $X^T X$ is square, we *could* just left-multiply both sides by its inverse to get an explicit
closed form for $\hat{\Theta}$:

\[ \hat{\Theta} = (X^T X)^{-1} y^T X \]

It turns out there is a faster and more numerically stable way (although obviously theoretically
equivalent) of solving for $\hat{\Theta}$ which relies on the [QR Decomposition][QRD] of the matrix $X^T X$.

QR Decomposition
----------------

Since we're going to be implementing QR in a minute, it's worth making sure we understand how it works in
detail. A QR decomposition of a matrix square $A$ is a product of an orthogonal matrix $Q$ and an upper-triangular
matrix $R$ such that $A = QR$. Why is this beneficial? Well, it turns out that to invert an orthogonal matrix,
all you need to do is take its transpose. $Q^T Q = \mathbb{1} \leftrightarrow Q^T = Q^{-1}$. So if we found
$QR = X^T X$, we could write our equation for $\hat{\Theta}$ as

\[ QR \hat{\Theta} = y^T X \]

\[ R \hat{\Theta} = Q^T y^T X \]

But R is upper triangular, and the right hand-side reduces to a single column vector, so we can solve this by
back-substitution. Back-substitution is easiest to explain with an example. Consider this system
of equations in matrix form, where the matrix is upper-triangular:

\[
 \begin{bmatrix}
   2 & 1 & 3 \\\
   0 & 1 & 1 \\
   0 & 0 & 4 \\
  \end{bmatrix}
 \begin{bmatrix}
   x_1 \\
   x_2 \\
   x_3 \\
  \end{bmatrix}
  =
 \begin{bmatrix}
   2 \\
   2 \\
   8 \\
  \end{bmatrix}
\]

We start on the bottom row, which is simply an equation
$4x_3 = 8$, so $x_3 = 2$. The second row represents the equation $x_2 + x_3 =2$, but we already know $x_3$,
so we can substitute that back in to get $x_2 - 2 = 0$, so $x_2 = 0$. The top row is $2x_1 + x_2 + 3_x3 = 2x_1 + 6 = 2$, so $x_1 = -2$.
This is back-substitution, and it should be clear that we can do this quickly and efficiently for an upper-triangular
matrix of any size. Furthermore, because we do at most one division per row, this method is very numerically stable.
(If the matrix is [ill-conditioned][IC], we could still run into numerical error, but this only occurs when the original
data set $X$ suffers from [multicolinearity][MC].)

[IC]: https://en.wikipedia.org/wiki/Condition_number
[MC]: https://en.wikipedia.org/wiki/Multicollinearity

So we know know how to finish the problem once we have $QR = X^X$. But how to get $Q$ and $R$? There are two
parts to understanding the algorithm. First, note that the product of any two orthogonal matrices is itself
orthogonal. Also, the identity matrix is orthogonal. Therefore, if we have a
candidate decomposition $A = QR$ where $Q$ is orthogonal (but R may not yet be square), then for any orthogonal
matrix $S$ we have $A = Q I R = Q S^T S R = (Q S^T) (S R) = Q' R'$ where $Q' = Q S^T$ and $R' = S R$ is *also*
a candidate decomposition! This is the general strategy behind not just QR decomposition, but behind many other
decompositions in linear algebra: at each step we want to apply an orthogonal transformation to bring $A$ closer
to the desired form, while simultaneously keeping track of a matrix to undo all of the transformations. 

That sets the rules and the goal of the game: we can apply any sequence of
orthogonal transforms to a (square, non-singular) matrix $A$ that will bring it
into upper triangular form. But is there such a sequence? And how would it be
constructed?

Householder Reflections
-----------------------

Let's break it down into an even easier problem first.
How would I make *just one column* of $A$ zero below the diagonal? Or even more concretely, how would I 
make just the *first column* of $A$ zero except for the first element? 

Let's take a look at the "column view" of our matrix. It looks like this:

\[
\begin{bmatrix}
    \vert & \vert & \vert \\
    a_1   & a_2 & a_3 \\
    \vert & \vert & \vert
\end{bmatrix}
\]

We want $a_1$ to be zero except for the first element. What does that *mean*? Let's call our basis
vectors $e_1 = [1\, 0\, 0]^T$, $e_2 = [0\, 1\, 0]^T$, $e_3 = [0\, 0\, 1]^T$. Every vector in our space is a
linear combination of these basis vectors. So what it means for $a_1$ to be zero except for the
first element is that $a_1$ is co-linear (in the same line) as 
$e_1$: \( H a_i = \alpha e_i \).

We're going to do this with an orthogonal transformation. But orthogonal transformations are *length preserving*.
That means $\alpha = ||a_1||$. So we now understand that we need to find an orthogonal matrix that sends
the vector $a_1$ to the vector $||a_1|| e_1$. Note that any two vectors lie in a plane. We could either
*rotate* by the angle between the vectors \(\cos^{-1} \frac{a_1 \dot e_1}{||a_1||} \), or we can *reflect*
across the line which bisects the two vectors in their plane. These two strategies are called the
[Givens rotation][GOR] and the [Householder reflection][HOR] respectively. The rotation matrix is slightly
less stable, so we will use the Householder reflection.

This diagram from Wikipedia illustrates this beautifully. Stare at it until you can see that reflecting
about the dotted plane sends $x$ to $||x||e_1$, and *believe* that $v$ is a unit vector orthogonal
to the dotted plane of reflection.

<a title="Bruguiea [CC BY-SA 3.0 (https://creativecommons.org/licenses/by-sa/3.0)], from Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File:Householder.svg"><img width="256" alt="Householder" src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/7b/Householder.svg/256px-Householder.svg.png"></a>

Let's say that $v$ is the unit normal vector of a plane; how would I reflect an arbitary vector $x$ across that plane?
Well, if we subtracted $\langle x, v \rangle v$ from $x$, that would be a projection into the plane, right? So if we just
keep going the same direction and for the same distance again, we'll end up a point on the other side of the plane $x'$.
Both $x$ and $x'$ project to the same point on the plane, and furthermore both are a distance $\langle x, v \rangle$ from
the plane. In other words, this operation is a reflection. 

Because a reflection is a linear transformation, we can express it as a matrix, which we will call $H$. Here is how we
go from our geometric intuition to a matrix:

\[
	\begin{align}
	H x & \triangleq x - 2 \langle x, v \rangle v \\
		& = x - 2v \langle x, v \rangle \\
		& = Ix - 2 v (v^T x) \\
		& = Ix - 2 (v v^T) x \\
		& = (I - 2 (v v^T)) x 
	\end{align}
\]

Here, note that $v v^T$ is the *outer* product of $v$ with itself so is a square matrix with elements $v_i v_j$.


Implementing the Lemmas
-----------------------

Given the above theoretical presentation, and copious comments, I hope you will now be able to read and understand the following code:

	def householder_reflection(a, e):
		'''
		Given a vector a and a unit vector e,
		(where a is non-zero and not collinear with e)
		returns an orthogonal matrix which maps a
		into the line of e.
		'''
		
		# better safe than sorry.
		a = a.flatten()
		e = e / norm(e)  
		
		# a and norm(a) * e are of equal length so
		# form an isosceles triangle. Therefore the third side
		# of the triangle is perpendicular to the line
		# that bisects the angle between a and e. This third
		# side is given by a - ||a|| e, which we will call u.
		# Since u lies in the plane spanned by a and e
		# its clear that u is actually orthogonal to a plane
		# equadistant to both a and ||a|| e. This is our
		# plane of reflection. We normalize u to v to 
		# because a unit vector is required in the next step.
		u = a - np.sign(a[0]) * norm(a) * e  
		v = u / norm(u)
		
		# matrix form of the reflection:
		# x - 2<x, v>v ==
		# (I - 2v v^T) x == H x
		H = np.eye(len(a)) - 2 * np.outer(v, v)
		
		return H

With the householder reflection in hand, we can implement an iterative
version of the QR decomposition algorithm, using the Householder reflection
on each column in turn to transform `A` into an upper triangular matrix.

	def qr_decomposition(A):
		'''
		Given an n x n invertable matrix A, returns the pair:
			Q an orthogonal n x n matrix
			R an upper triangular n x n matrix
		such that QR = A.
		'''
		
		N = A.shape[0]
		
		# Q starts as a simple identity matrix.
		# R is not yet upper-triangular, but will be.
		Q = np.eye(N)
		R = A.copy()
		
		for i in range(N-1):
			# we don't actually need to construct it,
			# but conceptually we're working to update
			# the minor matrix R[i:, i:] during the i-th
			# iteration. We can stop at N-1 because the
			# N-th column of an NxN matrix already has
			# the pivot element on the last row with
			# nothing below that needs to be zeroed out. 
			
			# the first column vector of the minor matrix.
			r = R[i:, i]
			
			# if r and e are already co-linear then we won't
			# be able to construct the householder matrix,
			# but the good news is we won't need to!
			if np.allclose(r[1:], 0):
				continue
				
			# e is "e hat sub-i," the i-th basis vector of
			# the minor matrix.
			e = np.zeros(N-i)
			e[0] = 1  
			
			# The householder reflection is only
			# applied to the minor matrix - the
			# rest of the matrix is left unchanged,
			# which we represent with an identity matrix.
			# Note that means H is in block diagonal form
			# where every block is orthogonal, therefore H
			# itself is orthogonal.
			H = np.eye(N)
			H[i:, i:] = householder_reflection(r, e)

			# QR = A is invariant. Proof:
			# QR = A, H^T H = I => 
			# Q H^T H R = A =>
			# Q' = Q H^T, R' = H R =>
			# Q' R' = A. QED.
			#
			# By construction, the first column of the 
			# minor matrix now has zeros for all
			# subdiagonal matrix. By induction, we 
			# have that all subdiagonal elements in
			# columns j<=i are zero. When i=N, R
			# is upper triangular. 
			Q = Q @ H.T
			R = H @ R
		
		return Q, R

The last piece of the puzzle is back-substitution. This is straight-forward and
available in standard libraries, but to comply with the letter-of-law of the
"from scratch" challenge we'll implement our own version.

	def solve_triangular(A, b):
		'''
		Solves the equation Ax = b when A is an upper-triangular square matrix
		and b is a one dimensional vector by back-substitution. The length of b
		and the number of rows must match. Returns x as a one-dimensional numpy.ndarray
		of the same length as b.
		
		This isn't as micro-optimized as scipy.linalg.solve_triangular() but the
		algorithm is the same, and the asymptotic time complexity is the same.  
		'''
		
		# starting at the bottom, the last row is just a_N_N * x = b_N
		x = b[-1:] / A[-1,-1]
		
		for i in range(A.shape[0] - 2, -1, -1):
			back_substitutions = np.dot(A[i, (i+1):], x)
			rhs = b[i] - back_substitutions
			x_i = rhs / A[i, i]  # possible ill-conditioning warning?
			x = np.insert(x, 0, x_i)
	  
		return x

I won't lie - that was a ton of linear algebra we just ploughed through. If you
got through it, or if you had the good sense to skim ahead until you found
something that made sense, congratulations.

Before we move on to actually *using* our new functions, let's spend some
time making sure everything up to this point is correct.

	class QRTestCase(unittest.TestCase):
		'''
		Unit tests for QR decomposition and its dependencies. 
		'''
		
		def test_2d(self):
			A = np.array([[1,1], [0,1]])
			b = np.array([2,3])
			x = solve_triangular(A, b)
			assert_allclose(x, np.array([-1, 3]))
		
		def test_solve_triangular(self):
			for N in range(1, 20):
				A = np.triu(np.random.normal(size=(N, N)))
				x = np.random.normal(size=(N,))
				b = A @ x
				x2 = solve_triangular(A, b)
				assert_allclose(x, x2, atol=1e-5)
		
		def test_reflection(self):
			x = np.array([1,1,1])
			e1 = np.array([1,0,0])
			H = householder_reflection(x, e1)
			assert_allclose(H @ (sqrt(3)* np.array([1, 0, 0])), x, atol=1e-5)
			assert_allclose(H @ np.array([1,1,1]), sqrt(3) * e1, atol=1e-5)
		
		def test_qr(self):
			# already upper triangular
			A = np.array([[2,1], [0, 3]])
			Q, R = qr_decomposition(A)
			assert_allclose(Q, np.eye(2))
			assert_allclose(R, A)
			
			N = 3
			Q = ortho_group.rvs(N) # generates random orthogonal matrices
			R = np.triu(np.random.normal(size=(N, N)))
			A = Q @ R
			Q2, R2 = qr_decomposition(Q @ R)
			# note that QR is not quite unique, so we can't
			# just test Q == Q2, unfortunately.
			assert_allclose(Q2 @ R2, Q @ R, atol=1e-5)
			assert_allclose(np.abs(det(Q2)), 1.0, atol=1e-5)
			assert_allclose(R2[2, 0], 0, atol=1e-5)
			assert_allclose(R2[2, 1], 0, atol=1e-5)
			assert_allclose(R2[1, 0], 0, atol=1e-5)

With these tools in hand, we're ready to tackle linear regression proper.

Implementing Linear Regression
------------------------------

The final algorithm is deceptively simple. Compare the normal
equations derived above to the final two lines of the `fit()` method.

	class LinearRegression:
		def __init__(self, add_intercept=True):
			self.add_intercept = bool(add_intercept)

		def _design_matrix(self, X):
			if self.add_intercept:
				X = np.hstack([ np.ones((X.shape[0], 1)), X])
			return X

		def fit(self, X, y):
			X = self._design_matrix(X)
			
			# We need to solve the normal equations: 
			#    (X^T X) Theta_hat = X^T y
			# Let QR = (X^T X), giving 
			#     QR Theta_hat = X^T y
			# Q is orthogonal, so we can left multiply by Q^T
			#     R Theta_hat = Q^T X^T y
			# The right hand side is a vector and because R is
			# triangular, we can quickly solve for Theta_hat.         
			Q, R = qr_decomposition(X.T @ X)
			self.theta_hat = solve_triangular(R, Q.T @ X.T @ y)
		
		def predict(self, X):
			X = self._design_matrix(X)
			
			return X @ self.theta_hat
    
Note that while we follow the scikit-learn naming conventions,
we haven't imported anything from sklearn yet. All the code
up to this point would work fine in an environment where the
sklearn package wasn't even present. However, to *test* the
code, we are going to use a few sklearn and scipy dependencies.

Testing
-------

Let's first grab a bunch of test-only dependencies and also
grab a copy of the famous [Boston][BD] so we have a simple
regression problem to play with.

[BD]: https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html

	# testing purposes only
	from sklearn.datasets import load_boston
	import matplotlib
	from matplotlib import pyplot as plt
	%matplotlib inline
	from numpy.linalg import det
	from scipy.stats import ortho_group
	import unittest
	from numpy.testing import assert_allclose

	boston = load_boston()
	X_raw = boston.data
	y_raw = boston.target

	# shuffle the data to randomize the train/test split
	shuffle = np.random.permutation(len(y_raw))
	X_full = X_raw[shuffle].copy()
	y_full = y_raw[shuffle].copy()

	# 80/20 train/test split. 
	train_test_split = int(0.8 * len(y_full))
	X_train = X_full[:train_test_split, :]
	y_train = y_full[:train_test_split]
	X_test = X_full[train_test_split:, :]
	y_test = y_full[train_test_split:]

The model is fit to the training set only. If it fits the
training set pretty well we know it has learned the examples
we gave it; if it *also* fits the test set pretty well,
we know it's done more than just memorize the examples given
but has also learned a more general lesson that it can apply
to novel data that it's never seen before.

A good way to visualize model performance is to plot $y$ vs. $\hat{y}$ - in
other words, actual vs predicted. A perfect predictor would be a 45&deg; diagonal
through the origin; random guessing would be a shapeless or circular cloud of points. 

	model = LinearRegression()
	model.fit(X_train, y_train)

	def goodness_of_fit_report(label, model, X, y):
		y_hat = model.predict(X)
		
		# predicted-vs-actual plot
		plt.scatter(x=y, y=y_hat, label=label, alpha=0.5)
		plt.title("Predicted vs. Actual")
		plt.xlabel("Actual")
		plt.ylabel("Predictions")
		plt.legend()
		
		mse = np.mean( (y - y_hat)**2 )
		y_bar = np.mean(y)
		r2 = 1 - np.sum( (y-y_hat)**2 ) / np.sum( (y-y_bar)**2 )
		print("{label: <16} mse={mse:.2f}     r2={r2:.2f}".format(**locals()))
			
	plt.figure(figsize=(16,6))
	plt.subplot(1, 2, 1)
	goodness_of_fit_report("Training Set", model, X_train, y_train)
	plt.subplot(1, 2, 2)
	goodness_of_fit_report("Test Set", model, X_test, y_test)

> Training Set     mse=21.30     r2=0.73

> Test Set         mse=25.12     r2=0.75

And in point of fact this linear regression model does reasonably well on both
the train and test set, with correlation scores around 75%. That means it's
able to explain about three-quarters of the variation that it finds in $y$
from what it was able to learn about the relationship between $X$ and $y$.

It's also nice to visualize both actual and predicted values as a function of
the independent variable - but in this case $X$ is 13-dimensional so we will
simply a few random dimensions to visualize. If the model has learned anything
real about the relationship between $X$ and $y$, we should see two similar
clouds of points for actual $y$ and predicted $\hat{y}$. 

![Prediction vs. Actual Scatterplot, training set and test set](/post/ml-from-scratch-part-1-linear-regression_files/predicted_vs_actual.png)

	y_hat = model.predict(X_train)
	plt.figure(figsize=(16,32))
	for i in range(4, 8):
		plt.subplot(6, 2, i+1)
		plt.scatter(x=X_train[:, i], y=y_train, alpha=0.2, label='Actual')
		plt.scatter(x=X_train[:, i], y=y_hat, alpha=0.2, label='Predicted')
		plt.legend()
		plt.xlabel(boston.feature_names[i])
		plt.ylabel("Response Variable")

![Predicted vs. Actual over pairs of independent variables](/post/ml-from-scratch-part-1-linear-regression_files/scatter.png)

Conclusion
----------

That was linear regression from scratch. There's a lot more that could be said about linear regression
even as a blackbox predictive model: polynomial and interaction terms, L1 and L2 regularization, and linear constraints on coefficients.

There's also a whole universe of techniques for doing statistical modeling and inference with linear regression: testing residuals
for homoskedacity, normality, autocorrelation, variance inflation factors, orthogonal polynomial regression, Cook's distance, 
leverage, studentized residuals, ANOVA, AIC, BIC, Omnibus F-tests on nested models. Just to be clear, these aren't *variations*
or *generalization* of linear regression (although there are ton of those too) these are just standard techniques for analyzing
and understanding linear regression models of the exact same form we calculated above. The topic is very mature with a corresponding
amount of associated mathematical machinery built up over the centuries.





[GOR]: https://en.wikipedia.org/wiki/Givens_rotation
[HOR]: https://en.wikipedia.org/wiki/Householder_transformation
[QFG]: https://www.cs.ubc.ca/~schmidtm/Courses/340-F16/linearQuadraticGradients.pdf
[GMT]: https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem
[MLE]: http://mathworld.wolfram.com/MaximumLikelihood.html
[NEQ]: http://mathworld.wolfram.com/NormalEquation.html
[QRD]: https://en.wikipedia.org/wiki/QR_decomposition
