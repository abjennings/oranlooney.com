---
title: "Your First Two R Packages"
author: "Oran Looney"
date: 2018-06-16T16:27:14:00-06:00
categories: ["R"]
tags: ["tidyverse", "ggplot2"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse=TRUE)
library(tidyverse)
library(ggplot2)
library(GGally)
data("midwest", package="ggplot2")
```

# Don't Start With Vanilla!

It's natural to assume you should learn the built-in libraries of a new language before
moving on to third-party libraries. Unfortunately, when it comes to R, there are few
key areas where the built-in functionality is so terrible, and so out-dated, and so full
of impossible-to-remember abbreviations, and frankly not very good even once you *do* learn
it, that it's best to start with a few high quality libraries and treat the built-in functionality
as advanced functionalty that you'll come back and learn when you need to interface with legacy
code. Those two areas are:

1. Data Manipulation
2. Visualization

Wait a minute, you might say. Aren't those both fundamental to applied statistics and data mining?
If R has such a good reputation, how can it be absolutely terrible at those two things?

Well, it's really just R's *built-in* utilities that are terrible. And not
really that terrible in an absolute sense - I'm sure they were killer circa 1990
- just terrible in comparison to modern R packages or other modern frameworks like Pandas. It's 
generally accepted in R development that the built-ins are just there for backwards compatability
with legacy code (stretching back decades) and to get anything reasonable, you install a package.

There are actually multiple options for improving R's built-in `data.frame`, but the one I recommend
is `dplyr`, which actually comes bundled in a package called the `tidyverse` containing many
useful utilities.

```{r}
mw_race <- midwest %>% 
  transmute(
    state, 
    county, 
    #popwhite = popwhite / poptotal, 
    popblack = popblack / poptotal, 
    popamerindian = popamerindian / poptotal, 
    popasian = popasian / poptotal, 
    popother = popother / poptotal
  ) %>%
  gather(race, pop, -state, -county)
```

We can also form some nice plots:

```{r}
ggplot(mw_race, aes(race, pop, fill=race)) + 
  scale_y_log10(
  	labels = scales::percent) +
  geom_boxplot()
```

```{r}
ggplot(midwest, aes(x=percblack/100, y=percbelowpoverty/100)) +
  scale_x_log10(breaks=c(0.0001, 0.001, 0.01, 0.1, 1, 10, 100), labels = scales::percent) +
  scale_y_continuous(labels = scales::percent) + 
  geom_point(aes(size=log(poptotal), col=state, alpha=0.5)) + 
  scale_size(range=c(0.5, 4)) +
  geom_smooth(method="lm") +
  labs(title="Poverty Line and Race", x="Black Population (%)", y="Below Poverty Line (%)")
```

One handy utility actually visualizes an entire `data.frame` and all pairwise interactions 
between columns:

```{r}
GGally::ggpairs(iris, mapping=ggplot2::aes(color=Species, alpha=0.5))
```

This is more than just a pretty picture: it tells you almost everything you would need to
know about the distribution of each column (along the diagonal) as well as giving you
*two* separate visual representations of the relationship between each pair of variables.
Usually you get a correlation in the upper triangle and a scatter plot in the lower triangle
of the matrix, but `ggpairs()` is also aware of which data types are categorical factors
and will use an appropriate visualization, such as a bar chart or boxplot, to help pack
as much useful information into one grid as in mechanically possible.

