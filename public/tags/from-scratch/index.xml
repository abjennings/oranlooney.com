<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>From Scratch on OranLooney.com</title>
    <link>http://www.oranlooney.com/tags/from-scratch/</link>
    <description>Recent content in From Scratch on OranLooney.com</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>&amp;copy; Copyright {year} Oran Looney</copyright>
    <lastBuildDate>Sun, 03 Feb 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="http://www.oranlooney.com/tags/from-scratch/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>ML From Scratch, Part 3: Backpropagation</title>
      <link>http://www.oranlooney.com/post/ml-from-scratch-part-3-backpropagation/</link>
      <pubDate>Sun, 03 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>http://www.oranlooney.com/post/ml-from-scratch-part-3-backpropagation/</guid>
      <description>In today’s installment of Machine Learning From Scratch we’ll build on the logistic regression from last time to create a classifier which is able to automatically represent non-linear relationships and interactions between features: the neural network. In particular I want to focus on one central algorithm which allows us to apply gradient descent to deep neural networks: the backpropagation algorithm. The history of this algorithm appears to be somewhat complex (as you can hear from Yann LeCun himself in this 2018 interview) but luckily for us the algorithm in its modern form is not difficult - although it does require a solid handle on linear algebra and calculus.</description>
    </item>
    
    <item>
      <title>ML From Scratch, Part 2: Logistic Regression</title>
      <link>http://www.oranlooney.com/post/ml-from-scratch-part-2-logistic-regression/</link>
      <pubDate>Thu, 27 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.oranlooney.com/post/ml-from-scratch-part-2-logistic-regression/</guid>
      <description>In this second installment of the machine learning from scratch we switch the point of view from regression to classification: instead of estimating a number, we will be trying to guess which of 2 possible classes a given input belongs to. A modern example is looking at a photo and deciding if its a cat or a dog.
In practice, its extremely common to need to decide between \(k\) classes where \(k &amp;gt; 2\) but in this article we’ll limit ourselves to just two classes - the so-called binary classification problem - because generalizations to many classes are usually both tedious and straight-forward.</description>
    </item>
    
    <item>
      <title>ML From Scratch, Part 1: Linear Regression</title>
      <link>http://www.oranlooney.com/post/ml-from-scratch-part-1-linear-regression/</link>
      <pubDate>Thu, 29 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.oranlooney.com/post/ml-from-scratch-part-1-linear-regression/</guid>
      <description>To kick off this series, will start with something simple yet foundational: linear regression via ordinary least squares.
While not exciting, linear regression finds widespread use both as a standalone learning algorithm and as a building block in more advanced learning algorithms. For example, the output layer of a deep neural network trained for regression with MSE loss, simple AR time series models, and the “local regression” part of LOWESS smoothing are all (modified) examples of linear regression.</description>
    </item>
    
    <item>
      <title>ML From Scratch, Part 0: Introduction</title>
      <link>http://www.oranlooney.com/post/ml-from-scratch-part-0-introduction/</link>
      <pubDate>Sun, 11 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.oranlooney.com/post/ml-from-scratch-part-0-introduction/</guid>
      <description>Motivation“As an apprentice, every new magician must prove to his own satisfaction, at least once, that there is truly great power in magic.” - The Flying Sorcerers, by David Gerrold and Larry Niven
How do you know if you really understand something? You could just rely on the subjective experience of feeling like you understand. This sounds plausible - surely you of all people should know, right? But this runs head-first into in the Dunning-Kruger effect.</description>
    </item>
    
  </channel>
</rss>